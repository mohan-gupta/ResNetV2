digraph {
	graph [size="154.65,154.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2205711506704 [label="
 (1000)" fillcolor=darkolivegreen1]
	2205711476192 [label=AddBackward0]
	2205711476288 -> 2205711476192
	2205711476288 [label=SqueezeBackward3]
	2205711476096 -> 2205711476288
	2205711476096 [label=MmBackward0]
	2205711476384 -> 2205711476096
	2205711476384 [label=UnsqueezeBackward0]
	2205711476528 -> 2205711476384
	2205711476528 [label=ViewBackward0]
	2205711476624 -> 2205711476528
	2205711476624 [label=MeanBackward1]
	2205711476720 -> 2205711476624
	2205711476720 [label=ReluBackward0]
	2205711476816 -> 2205711476720
	2205711476816 [label=NativeBatchNormBackward0]
	2205711476912 -> 2205711476816
	2205711476912 [label=AddBackward0]
	2205711477104 -> 2205711476912
	2205711477104 [label=ConvolutionBackward0]
	2205711477248 -> 2205711477104
	2205711477248 [label=ReluBackward0]
	2205711477392 -> 2205711477248
	2205711477392 [label=NativeBatchNormBackward0]
	2205711477488 -> 2205711477392
	2205711477488 [label=ConvolutionBackward0]
	2205711477680 -> 2205711477488
	2205711477680 [label=ReluBackward0]
	2205711477824 -> 2205711477680
	2205711477824 [label=NativeBatchNormBackward0]
	2205711477920 -> 2205711477824
	2205711477920 [label=ConvolutionBackward0]
	2205711478112 -> 2205711477920
	2205711478112 [label=ReluBackward0]
	2205711478256 -> 2205711478112
	2205711478256 [label=NativeBatchNormBackward0]
	2205711477056 -> 2205711478256
	2205711477056 [label=AddBackward0]
	2205711478496 -> 2205711477056
	2205711478496 [label=ConvolutionBackward0]
	2205711478640 -> 2205711478496
	2205711478640 [label=ReluBackward0]
	2205711478736 -> 2205711478640
	2205711478736 [label=NativeBatchNormBackward0]
	2205711560864 -> 2205711478736
	2205711560864 [label=ConvolutionBackward0]
	2205711561056 -> 2205711560864
	2205711561056 [label=ReluBackward0]
	2205711561200 -> 2205711561056
	2205711561200 [label=NativeBatchNormBackward0]
	2205711561344 -> 2205711561200
	2205711561344 [label=ConvolutionBackward0]
	2205711561536 -> 2205711561344
	2205711561536 [label=ReluBackward0]
	2205711561680 -> 2205711561536
	2205711561680 [label=NativeBatchNormBackward0]
	2205711478448 -> 2205711561680
	2205711478448 [label=AddBackward0]
	2205711561968 -> 2205711478448
	2205711561968 [label=ConvolutionBackward0]
	2205711562112 -> 2205711561968
	2205711562112 [label=ReluBackward0]
	2205711562256 -> 2205711562112
	2205711562256 [label=NativeBatchNormBackward0]
	2205711562400 -> 2205711562256
	2205711562400 [label=ConvolutionBackward0]
	2205711562592 -> 2205711562400
	2205711562592 [label=ReluBackward0]
	2205711562736 -> 2205711562592
	2205711562736 [label=NativeBatchNormBackward0]
	2205711562880 -> 2205711562736
	2205711562880 [label=ConvolutionBackward0]
	2205711563072 -> 2205711562880
	2205711563072 [label=ReluBackward0]
	2205711563216 -> 2205711563072
	2205711563216 [label=NativeBatchNormBackward0]
	2205711563360 -> 2205711563216
	2205711563360 [label=AddBackward0]
	2205711563552 -> 2205711563360
	2205711563552 [label=ConvolutionBackward0]
	2205711563696 -> 2205711563552
	2205711563696 [label=ReluBackward0]
	2205711563840 -> 2205711563696
	2205711563840 [label=NativeBatchNormBackward0]
	2205711563984 -> 2205711563840
	2205711563984 [label=ConvolutionBackward0]
	2205711564176 -> 2205711563984
	2205711564176 [label=ReluBackward0]
	2205711564320 -> 2205711564176
	2205711564320 [label=NativeBatchNormBackward0]
	2205711564464 -> 2205711564320
	2205711564464 [label=ConvolutionBackward0]
	2205711564656 -> 2205711564464
	2205711564656 [label=ReluBackward0]
	2205711564752 -> 2205711564656
	2205711564752 [label=NativeBatchNormBackward0]
	2205711563504 -> 2205711564752
	2205711563504 [label=AddBackward0]
	2205711573344 -> 2205711563504
	2205711573344 [label=ConvolutionBackward0]
	2205711573488 -> 2205711573344
	2205711573488 [label=ReluBackward0]
	2205711573632 -> 2205711573488
	2205711573632 [label=NativeBatchNormBackward0]
	2205711573776 -> 2205711573632
	2205711573776 [label=ConvolutionBackward0]
	2205711573968 -> 2205711573776
	2205711573968 [label=ReluBackward0]
	2205711574112 -> 2205711573968
	2205711574112 [label=NativeBatchNormBackward0]
	2205711574256 -> 2205711574112
	2205711574256 [label=ConvolutionBackward0]
	2205711574448 -> 2205711574256
	2205711574448 [label=ReluBackward0]
	2205711574592 -> 2205711574448
	2205711574592 [label=NativeBatchNormBackward0]
	2205711573296 -> 2205711574592
	2205711573296 [label=AddBackward0]
	2205711574880 -> 2205711573296
	2205711574880 [label=ConvolutionBackward0]
	2205711575024 -> 2205711574880
	2205711575024 [label=ReluBackward0]
	2205711575168 -> 2205711575024
	2205711575168 [label=NativeBatchNormBackward0]
	2205711575312 -> 2205711575168
	2205711575312 [label=ConvolutionBackward0]
	2205711575504 -> 2205711575312
	2205711575504 [label=ReluBackward0]
	2205711575648 -> 2205711575504
	2205711575648 [label=NativeBatchNormBackward0]
	2205711575792 -> 2205711575648
	2205711575792 [label=ConvolutionBackward0]
	2205711575984 -> 2205711575792
	2205711575984 [label=ReluBackward0]
	2205711576128 -> 2205711575984
	2205711576128 [label=NativeBatchNormBackward0]
	2205711574832 -> 2205711576128
	2205711574832 [label=AddBackward0]
	2205711576416 -> 2205711574832
	2205711576416 [label=ConvolutionBackward0]
	2205711576560 -> 2205711576416
	2205711576560 [label=ReluBackward0]
	2205711576704 -> 2205711576560
	2205711576704 [label=NativeBatchNormBackward0]
	2205711576848 -> 2205711576704
	2205711576848 [label=ConvolutionBackward0]
	2205711577040 -> 2205711576848
	2205711577040 [label=ReluBackward0]
	2205711589536 -> 2205711577040
	2205711589536 [label=NativeBatchNormBackward0]
	2205711589680 -> 2205711589536
	2205711589680 [label=ConvolutionBackward0]
	2205711589872 -> 2205711589680
	2205711589872 [label=ReluBackward0]
	2205711590016 -> 2205711589872
	2205711590016 [label=NativeBatchNormBackward0]
	2205711576368 -> 2205711590016
	2205711576368 [label=AddBackward0]
	2205711590304 -> 2205711576368
	2205711590304 [label=ConvolutionBackward0]
	2205711590448 -> 2205711590304
	2205711590448 [label=ReluBackward0]
	2205711590592 -> 2205711590448
	2205711590592 [label=NativeBatchNormBackward0]
	2205711590736 -> 2205711590592
	2205711590736 [label=ConvolutionBackward0]
	2205711590928 -> 2205711590736
	2205711590928 [label=ReluBackward0]
	2205711591072 -> 2205711590928
	2205711591072 [label=NativeBatchNormBackward0]
	2205711591216 -> 2205711591072
	2205711591216 [label=ConvolutionBackward0]
	2205711591408 -> 2205711591216
	2205711591408 [label=ReluBackward0]
	2205711591552 -> 2205711591408
	2205711591552 [label=NativeBatchNormBackward0]
	2205711590256 -> 2205711591552
	2205711590256 [label=AddBackward0]
	2205711591840 -> 2205711590256
	2205711591840 [label=ConvolutionBackward0]
	2205711591984 -> 2205711591840
	2205711591984 [label=ReluBackward0]
	2205711592128 -> 2205711591984
	2205711592128 [label=NativeBatchNormBackward0]
	2205711592272 -> 2205711592128
	2205711592272 [label=ConvolutionBackward0]
	2205711592464 -> 2205711592272
	2205711592464 [label=ReluBackward0]
	2205711592608 -> 2205711592464
	2205711592608 [label=NativeBatchNormBackward0]
	2205711592752 -> 2205711592608
	2205711592752 [label=ConvolutionBackward0]
	2205711592944 -> 2205711592752
	2205711592944 [label=ReluBackward0]
	2205711593088 -> 2205711592944
	2205711593088 [label=NativeBatchNormBackward0]
	2205711593232 -> 2205711593088
	2205711593232 [label=AddBackward0]
	2205711593424 -> 2205711593232
	2205711593424 [label=ConvolutionBackward0]
	2205711605920 -> 2205711593424
	2205711605920 [label=ReluBackward0]
	2205711606064 -> 2205711605920
	2205711606064 [label=NativeBatchNormBackward0]
	2205711606208 -> 2205711606064
	2205711606208 [label=ConvolutionBackward0]
	2205711606400 -> 2205711606208
	2205711606400 [label=ReluBackward0]
	2205711606544 -> 2205711606400
	2205711606544 [label=NativeBatchNormBackward0]
	2205711606688 -> 2205711606544
	2205711606688 [label=ConvolutionBackward0]
	2205711606880 -> 2205711606688
	2205711606880 [label=ReluBackward0]
	2205711607024 -> 2205711606880
	2205711607024 [label=NativeBatchNormBackward0]
	2205711593376 -> 2205711607024
	2205711593376 [label=AddBackward0]
	2205711607312 -> 2205711593376
	2205711607312 [label=ConvolutionBackward0]
	2205711607456 -> 2205711607312
	2205711607456 [label=ReluBackward0]
	2205711607600 -> 2205711607456
	2205711607600 [label=NativeBatchNormBackward0]
	2205711607744 -> 2205711607600
	2205711607744 [label=ConvolutionBackward0]
	2205711607936 -> 2205711607744
	2205711607936 [label=ReluBackward0]
	2205711608080 -> 2205711607936
	2205711608080 [label=NativeBatchNormBackward0]
	2205711608224 -> 2205711608080
	2205711608224 [label=ConvolutionBackward0]
	2205711608416 -> 2205711608224
	2205711608416 [label=ReluBackward0]
	2205711608560 -> 2205711608416
	2205711608560 [label=NativeBatchNormBackward0]
	2205711607264 -> 2205711608560
	2205711607264 [label=AddBackward0]
	2205711608848 -> 2205711607264
	2205711608848 [label=ConvolutionBackward0]
	2205711608992 -> 2205711608848
	2205711608992 [label=ReluBackward0]
	2205711609136 -> 2205711608992
	2205711609136 [label=NativeBatchNormBackward0]
	2205711609280 -> 2205711609136
	2205711609280 [label=ConvolutionBackward0]
	2205711609472 -> 2205711609280
	2205711609472 [label=ReluBackward0]
	2205711609616 -> 2205711609472
	2205711609616 [label=NativeBatchNormBackward0]
	2205711609760 -> 2205711609616
	2205711609760 [label=ConvolutionBackward0]
	2205711614112 -> 2205711609760
	2205711614112 [label=ReluBackward0]
	2205711614256 -> 2205711614112
	2205711614256 [label=NativeBatchNormBackward0]
	2205711608800 -> 2205711614256
	2205711608800 [label=AddBackward0]
	2205711614544 -> 2205711608800
	2205711614544 [label=ConvolutionBackward0]
	2205711614688 -> 2205711614544
	2205711614688 [label=ReluBackward0]
	2205711614832 -> 2205711614688
	2205711614832 [label=NativeBatchNormBackward0]
	2205711614976 -> 2205711614832
	2205711614976 [label=ConvolutionBackward0]
	2205711615168 -> 2205711614976
	2205711615168 [label=ReluBackward0]
	2205711615312 -> 2205711615168
	2205711615312 [label=NativeBatchNormBackward0]
	2205711615456 -> 2205711615312
	2205711615456 [label=ConvolutionBackward0]
	2205711615648 -> 2205711615456
	2205711615648 [label=ReluBackward0]
	2205711615792 -> 2205711615648
	2205711615792 [label=NativeBatchNormBackward0]
	2205711615936 -> 2205711615792
	2205711615936 [label=AddBackward0]
	2205711616128 -> 2205711615936
	2205711616128 [label=ConvolutionBackward0]
	2205711616272 -> 2205711616128
	2205711616272 [label=ReluBackward0]
	2205711616416 -> 2205711616272
	2205711616416 [label=NativeBatchNormBackward0]
	2205711616560 -> 2205711616416
	2205711616560 [label=ConvolutionBackward0]
	2205711616752 -> 2205711616560
	2205711616752 [label=ReluBackward0]
	2205711616896 -> 2205711616752
	2205711616896 [label=NativeBatchNormBackward0]
	2205711617040 -> 2205711616896
	2205711617040 [label=ConvolutionBackward0]
	2205711617232 -> 2205711617040
	2205711617232 [label=ReluBackward0]
	2205711617376 -> 2205711617232
	2205711617376 [label=NativeBatchNormBackward0]
	2205711616080 -> 2205711617376
	2205711616080 [label=AddBackward0]
	2205711617664 -> 2205711616080
	2205711617664 [label=ConvolutionBackward0]
	2205711617808 -> 2205711617664
	2205711617808 [label=ReluBackward0]
	2205711617952 -> 2205711617808
	2205711617952 [label=NativeBatchNormBackward0]
	2205711618000 -> 2205711617952
	2205711618000 [label=ConvolutionBackward0]
	2205711634736 -> 2205711618000
	2205711634736 [label=ReluBackward0]
	2205711634880 -> 2205711634736
	2205711634880 [label=NativeBatchNormBackward0]
	2205711635024 -> 2205711634880
	2205711635024 [label=ConvolutionBackward0]
	2205711635216 -> 2205711635024
	2205711635216 [label=ReluBackward0]
	2205711635360 -> 2205711635216
	2205711635360 [label=NativeBatchNormBackward0]
	2205711617616 -> 2205711635360
	2205711617616 [label=AddBackward0]
	2205711635648 -> 2205711617616
	2205711635648 [label=ConvolutionBackward0]
	2205711635792 -> 2205711635648
	2205711635792 [label=ReluBackward0]
	2205711635936 -> 2205711635792
	2205711635936 [label=NativeBatchNormBackward0]
	2205711636080 -> 2205711635936
	2205711636080 [label=ConvolutionBackward0]
	2205711636272 -> 2205711636080
	2205711636272 [label=ReluBackward0]
	2205711636416 -> 2205711636272
	2205711636416 [label=NativeBatchNormBackward0]
	2205711636560 -> 2205711636416
	2205711636560 [label=ConvolutionBackward0]
	2205711636752 -> 2205711636560
	2205711636752 [label=ReluBackward0]
	2205711636896 -> 2205711636752
	2205711636896 [label=NativeBatchNormBackward0]
	2205711637040 -> 2205711636896
	2205711637040 [label=MaxPool2DWithIndicesBackward0]
	2205711637232 -> 2205711637040
	2205711637232 [label=ConvolutionBackward0]
	2205711637376 -> 2205711637232
	2205639484416 [label="conv_1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	2205639484416 -> 2205711637376
	2205711637376 [label=AccumulateGrad]
	2205711637328 -> 2205711637232
	2205639484496 [label="conv_1.bias
 (64)" fillcolor=lightblue]
	2205639484496 -> 2205711637328
	2205711637328 [label=AccumulateGrad]
	2205711636992 -> 2205711636896
	2205639484576 [label="layer1.0.block.bn1.weight
 (64)" fillcolor=lightblue]
	2205639484576 -> 2205711636992
	2205711636992 [label=AccumulateGrad]
	2205711636944 -> 2205711636896
	2205639484656 [label="layer1.0.block.bn1.bias
 (64)" fillcolor=lightblue]
	2205639484656 -> 2205711636944
	2205711636944 [label=AccumulateGrad]
	2205711636704 -> 2205711636560
	2205639485136 [label="layer1.0.block.conv_x_1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2205639485136 -> 2205711636704
	2205711636704 [label=AccumulateGrad]
	2205711636512 -> 2205711636416
	2205639485216 [label="layer1.0.block.bn2.weight
 (64)" fillcolor=lightblue]
	2205639485216 -> 2205711636512
	2205711636512 [label=AccumulateGrad]
	2205711636464 -> 2205711636416
	2205639485296 [label="layer1.0.block.bn2.bias
 (64)" fillcolor=lightblue]
	2205639485296 -> 2205711636464
	2205711636464 [label=AccumulateGrad]
	2205711636224 -> 2205711636080
	2205639485696 [label="layer1.0.block.conv_x_2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2205639485696 -> 2205711636224
	2205711636224 [label=AccumulateGrad]
	2205711636032 -> 2205711635936
	2205639485776 [label="layer1.0.block.bn3.weight
 (64)" fillcolor=lightblue]
	2205639485776 -> 2205711636032
	2205711636032 [label=AccumulateGrad]
	2205711635984 -> 2205711635936
	2205639485856 [label="layer1.0.block.bn3.bias
 (64)" fillcolor=lightblue]
	2205639485856 -> 2205711635984
	2205711635984 [label=AccumulateGrad]
	2205711635744 -> 2205711635648
	2205639486256 [label="layer1.0.block.conv_x_3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2205639486256 -> 2205711635744
	2205711635744 [label=AccumulateGrad]
	2205711635600 -> 2205711617616
	2205711635600 [label=ConvolutionBackward0]
	2205711636128 -> 2205711635600
	2205711636128 [label=ReluBackward0]
	2205711636608 -> 2205711636128
	2205711636608 [label=NativeBatchNormBackward0]
	2205711637040 -> 2205711636608
	2205711637184 -> 2205711636608
	2205639486336 [label="layer1.0.bn_proj.weight
 (64)" fillcolor=lightblue]
	2205639486336 -> 2205711637184
	2205711637184 [label=AccumulateGrad]
	2205711636368 -> 2205711636608
	2205639486416 [label="layer1.0.bn_proj.bias
 (64)" fillcolor=lightblue]
	2205639486416 -> 2205711636368
	2205711636368 [label=AccumulateGrad]
	2205711636176 -> 2205711635600
	2205639486816 [label="layer1.0.projection.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2205639486816 -> 2205711636176
	2205711636176 [label=AccumulateGrad]
	2205711635504 -> 2205711635360
	2205639486896 [label="layer1.1.block.bn1.weight
 (256)" fillcolor=lightblue]
	2205639486896 -> 2205711635504
	2205711635504 [label=AccumulateGrad]
	2205711635456 -> 2205711635360
	2205639486976 [label="layer1.1.block.bn1.bias
 (256)" fillcolor=lightblue]
	2205639486976 -> 2205711635456
	2205711635456 [label=AccumulateGrad]
	2205711635168 -> 2205711635024
	2205639487376 [label="layer1.1.block.conv_x_1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2205639487376 -> 2205711635168
	2205711635168 [label=AccumulateGrad]
	2205711634976 -> 2205711634880
	2205639532608 [label="layer1.1.block.bn2.weight
 (64)" fillcolor=lightblue]
	2205639532608 -> 2205711634976
	2205711634976 [label=AccumulateGrad]
	2205711634928 -> 2205711634880
	2205639532688 [label="layer1.1.block.bn2.bias
 (64)" fillcolor=lightblue]
	2205639532688 -> 2205711634928
	2205711634928 [label=AccumulateGrad]
	2205711634688 -> 2205711618000
	2205639533088 [label="layer1.1.block.conv_x_2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2205639533088 -> 2205711634688
	2205711634688 [label=AccumulateGrad]
	2205711634544 -> 2205711617952
	2205639533168 [label="layer1.1.block.bn3.weight
 (64)" fillcolor=lightblue]
	2205639533168 -> 2205711634544
	2205711634544 [label=AccumulateGrad]
	2205711634496 -> 2205711617952
	2205639533248 [label="layer1.1.block.bn3.bias
 (64)" fillcolor=lightblue]
	2205639533248 -> 2205711634496
	2205711634496 [label=AccumulateGrad]
	2205711617760 -> 2205711617664
	2205639533648 [label="layer1.1.block.conv_x_3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2205639533648 -> 2205711617760
	2205711617760 [label=AccumulateGrad]
	2205711617616 -> 2205711616080
	2205711617520 -> 2205711617376
	2205639533728 [label="layer1.2.block.bn1.weight
 (256)" fillcolor=lightblue]
	2205639533728 -> 2205711617520
	2205711617520 [label=AccumulateGrad]
	2205711617472 -> 2205711617376
	2205639533808 [label="layer1.2.block.bn1.bias
 (256)" fillcolor=lightblue]
	2205639533808 -> 2205711617472
	2205711617472 [label=AccumulateGrad]
	2205711617184 -> 2205711617040
	2205639534208 [label="layer1.2.block.conv_x_1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2205639534208 -> 2205711617184
	2205711617184 [label=AccumulateGrad]
	2205711616992 -> 2205711616896
	2205639534288 [label="layer1.2.block.bn2.weight
 (64)" fillcolor=lightblue]
	2205639534288 -> 2205711616992
	2205711616992 [label=AccumulateGrad]
	2205711616944 -> 2205711616896
	2205639534368 [label="layer1.2.block.bn2.bias
 (64)" fillcolor=lightblue]
	2205639534368 -> 2205711616944
	2205711616944 [label=AccumulateGrad]
	2205711616704 -> 2205711616560
	2205639534768 [label="layer1.2.block.conv_x_2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2205639534768 -> 2205711616704
	2205711616704 [label=AccumulateGrad]
	2205711616512 -> 2205711616416
	2205639534848 [label="layer1.2.block.bn3.weight
 (64)" fillcolor=lightblue]
	2205639534848 -> 2205711616512
	2205711616512 [label=AccumulateGrad]
	2205711616464 -> 2205711616416
	2205639534928 [label="layer1.2.block.bn3.bias
 (64)" fillcolor=lightblue]
	2205639534928 -> 2205711616464
	2205711616464 [label=AccumulateGrad]
	2205711616224 -> 2205711616128
	2205639535328 [label="layer1.2.block.conv_x_3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2205639535328 -> 2205711616224
	2205711616224 [label=AccumulateGrad]
	2205711616080 -> 2205711615936
	2205711615888 -> 2205711615792
	2205639535408 [label="layer2.0.block.bn1.weight
 (256)" fillcolor=lightblue]
	2205639535408 -> 2205711615888
	2205711615888 [label=AccumulateGrad]
	2205711615840 -> 2205711615792
	2205639535488 [label="layer2.0.block.bn1.bias
 (256)" fillcolor=lightblue]
	2205639535488 -> 2205711615840
	2205711615840 [label=AccumulateGrad]
	2205711615600 -> 2205711615456
	2205639535888 [label="layer2.0.block.conv_x_1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	2205639535888 -> 2205711615600
	2205711615600 [label=AccumulateGrad]
	2205711615408 -> 2205711615312
	2205639535968 [label="layer2.0.block.bn2.weight
 (128)" fillcolor=lightblue]
	2205639535968 -> 2205711615408
	2205711615408 [label=AccumulateGrad]
	2205711615360 -> 2205711615312
	2205639536048 [label="layer2.0.block.bn2.bias
 (128)" fillcolor=lightblue]
	2205639536048 -> 2205711615360
	2205711615360 [label=AccumulateGrad]
	2205711615120 -> 2205711614976
	2205639536448 [label="layer2.0.block.conv_x_2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2205639536448 -> 2205711615120
	2205711615120 [label=AccumulateGrad]
	2205711614928 -> 2205711614832
	2205639536528 [label="layer2.0.block.bn3.weight
 (128)" fillcolor=lightblue]
	2205639536528 -> 2205711614928
	2205711614928 [label=AccumulateGrad]
	2205711614880 -> 2205711614832
	2205639602240 [label="layer2.0.block.bn3.bias
 (128)" fillcolor=lightblue]
	2205639602240 -> 2205711614880
	2205711614880 [label=AccumulateGrad]
	2205711614640 -> 2205711614544
	2205639602640 [label="layer2.0.block.conv_x_3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2205639602640 -> 2205711614640
	2205711614640 [label=AccumulateGrad]
	2205711614496 -> 2205711608800
	2205711614496 [label=ConvolutionBackward0]
	2205711615024 -> 2205711614496
	2205711615024 [label=ReluBackward0]
	2205711615504 -> 2205711615024
	2205711615504 [label=NativeBatchNormBackward0]
	2205711615936 -> 2205711615504
	2205711616032 -> 2205711615504
	2205639602720 [label="layer2.0.bn_proj.weight
 (256)" fillcolor=lightblue]
	2205639602720 -> 2205711616032
	2205711616032 [label=AccumulateGrad]
	2205711615264 -> 2205711615504
	2205639602800 [label="layer2.0.bn_proj.bias
 (256)" fillcolor=lightblue]
	2205639602800 -> 2205711615264
	2205711615264 [label=AccumulateGrad]
	2205711615072 -> 2205711614496
	2205639603200 [label="layer2.0.projection.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2205639603200 -> 2205711615072
	2205711615072 [label=AccumulateGrad]
	2205711614400 -> 2205711614256
	2205639603280 [label="layer2.1.block.bn1.weight
 (512)" fillcolor=lightblue]
	2205639603280 -> 2205711614400
	2205711614400 [label=AccumulateGrad]
	2205711614352 -> 2205711614256
	2205639603360 [label="layer2.1.block.bn1.bias
 (512)" fillcolor=lightblue]
	2205639603360 -> 2205711614352
	2205711614352 [label=AccumulateGrad]
	2205711614064 -> 2205711609760
	2205639603760 [label="layer2.1.block.conv_x_1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2205639603760 -> 2205711614064
	2205711614064 [label=AccumulateGrad]
	2205711609712 -> 2205711609616
	2205639603840 [label="layer2.1.block.bn2.weight
 (128)" fillcolor=lightblue]
	2205639603840 -> 2205711609712
	2205711609712 [label=AccumulateGrad]
	2205711609664 -> 2205711609616
	2205639603920 [label="layer2.1.block.bn2.bias
 (128)" fillcolor=lightblue]
	2205639603920 -> 2205711609664
	2205711609664 [label=AccumulateGrad]
	2205711609424 -> 2205711609280
	2205639604320 [label="layer2.1.block.conv_x_2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2205639604320 -> 2205711609424
	2205711609424 [label=AccumulateGrad]
	2205711609232 -> 2205711609136
	2205639604400 [label="layer2.1.block.bn3.weight
 (128)" fillcolor=lightblue]
	2205639604400 -> 2205711609232
	2205711609232 [label=AccumulateGrad]
	2205711609184 -> 2205711609136
	2205639604480 [label="layer2.1.block.bn3.bias
 (128)" fillcolor=lightblue]
	2205639604480 -> 2205711609184
	2205711609184 [label=AccumulateGrad]
	2205711608944 -> 2205711608848
	2205639604880 [label="layer2.1.block.conv_x_3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2205639604880 -> 2205711608944
	2205711608944 [label=AccumulateGrad]
	2205711608800 -> 2205711607264
	2205711608704 -> 2205711608560
	2205639604960 [label="layer2.2.block.bn1.weight
 (512)" fillcolor=lightblue]
	2205639604960 -> 2205711608704
	2205711608704 [label=AccumulateGrad]
	2205711608656 -> 2205711608560
	2205639605040 [label="layer2.2.block.bn1.bias
 (512)" fillcolor=lightblue]
	2205639605040 -> 2205711608656
	2205711608656 [label=AccumulateGrad]
	2205711608368 -> 2205711608224
	2205639605440 [label="layer2.2.block.conv_x_1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2205639605440 -> 2205711608368
	2205711608368 [label=AccumulateGrad]
	2205711608176 -> 2205711608080
	2205639605520 [label="layer2.2.block.bn2.weight
 (128)" fillcolor=lightblue]
	2205639605520 -> 2205711608176
	2205711608176 [label=AccumulateGrad]
	2205711608128 -> 2205711608080
	2205639605600 [label="layer2.2.block.bn2.bias
 (128)" fillcolor=lightblue]
	2205639605600 -> 2205711608128
	2205711608128 [label=AccumulateGrad]
	2205711607888 -> 2205711607744
	2205639606000 [label="layer2.2.block.conv_x_2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2205639606000 -> 2205711607888
	2205711607888 [label=AccumulateGrad]
	2205711607696 -> 2205711607600
	2205639606080 [label="layer2.2.block.bn3.weight
 (128)" fillcolor=lightblue]
	2205639606080 -> 2205711607696
	2205711607696 [label=AccumulateGrad]
	2205711607648 -> 2205711607600
	2205639606160 [label="layer2.2.block.bn3.bias
 (128)" fillcolor=lightblue]
	2205639606160 -> 2205711607648
	2205711607648 [label=AccumulateGrad]
	2205711607408 -> 2205711607312
	2205639668096 [label="layer2.2.block.conv_x_3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2205639668096 -> 2205711607408
	2205711607408 [label=AccumulateGrad]
	2205711607264 -> 2205711593376
	2205711607168 -> 2205711607024
	2205639668176 [label="layer2.3.block.bn1.weight
 (512)" fillcolor=lightblue]
	2205639668176 -> 2205711607168
	2205711607168 [label=AccumulateGrad]
	2205711607120 -> 2205711607024
	2205639668256 [label="layer2.3.block.bn1.bias
 (512)" fillcolor=lightblue]
	2205639668256 -> 2205711607120
	2205711607120 [label=AccumulateGrad]
	2205711606832 -> 2205711606688
	2205639668656 [label="layer2.3.block.conv_x_1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2205639668656 -> 2205711606832
	2205711606832 [label=AccumulateGrad]
	2205711606640 -> 2205711606544
	2205639668736 [label="layer2.3.block.bn2.weight
 (128)" fillcolor=lightblue]
	2205639668736 -> 2205711606640
	2205711606640 [label=AccumulateGrad]
	2205711606592 -> 2205711606544
	2205639668816 [label="layer2.3.block.bn2.bias
 (128)" fillcolor=lightblue]
	2205639668816 -> 2205711606592
	2205711606592 [label=AccumulateGrad]
	2205711606352 -> 2205711606208
	2205639669216 [label="layer2.3.block.conv_x_2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2205639669216 -> 2205711606352
	2205711606352 [label=AccumulateGrad]
	2205711606160 -> 2205711606064
	2205639669296 [label="layer2.3.block.bn3.weight
 (128)" fillcolor=lightblue]
	2205639669296 -> 2205711606160
	2205711606160 [label=AccumulateGrad]
	2205711606112 -> 2205711606064
	2205639669376 [label="layer2.3.block.bn3.bias
 (128)" fillcolor=lightblue]
	2205639669376 -> 2205711606112
	2205711606112 [label=AccumulateGrad]
	2205711605872 -> 2205711593424
	2205639669776 [label="layer2.3.block.conv_x_3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2205639669776 -> 2205711605872
	2205711605872 [label=AccumulateGrad]
	2205711593376 -> 2205711593232
	2205711593184 -> 2205711593088
	2205639669856 [label="layer3.0.block.bn1.weight
 (512)" fillcolor=lightblue]
	2205639669856 -> 2205711593184
	2205711593184 [label=AccumulateGrad]
	2205711593136 -> 2205711593088
	2205639669936 [label="layer3.0.block.bn1.bias
 (512)" fillcolor=lightblue]
	2205639669936 -> 2205711593136
	2205711593136 [label=AccumulateGrad]
	2205711592896 -> 2205711592752
	2205639670336 [label="layer3.0.block.conv_x_1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	2205639670336 -> 2205711592896
	2205711592896 [label=AccumulateGrad]
	2205711592704 -> 2205711592608
	2205639670416 [label="layer3.0.block.bn2.weight
 (256)" fillcolor=lightblue]
	2205639670416 -> 2205711592704
	2205711592704 [label=AccumulateGrad]
	2205711592656 -> 2205711592608
	2205639670496 [label="layer3.0.block.bn2.bias
 (256)" fillcolor=lightblue]
	2205639670496 -> 2205711592656
	2205711592656 [label=AccumulateGrad]
	2205711592416 -> 2205711592272
	2205639670896 [label="layer3.0.block.conv_x_2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2205639670896 -> 2205711592416
	2205711592416 [label=AccumulateGrad]
	2205711592224 -> 2205711592128
	2205639670976 [label="layer3.0.block.bn3.weight
 (256)" fillcolor=lightblue]
	2205639670976 -> 2205711592224
	2205711592224 [label=AccumulateGrad]
	2205711592176 -> 2205711592128
	2205639671056 [label="layer3.0.block.bn3.bias
 (256)" fillcolor=lightblue]
	2205639671056 -> 2205711592176
	2205711592176 [label=AccumulateGrad]
	2205711591936 -> 2205711591840
	2205639671456 [label="layer3.0.block.conv_x_3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2205639671456 -> 2205711591936
	2205711591936 [label=AccumulateGrad]
	2205711591792 -> 2205711590256
	2205711591792 [label=ConvolutionBackward0]
	2205711592320 -> 2205711591792
	2205711592320 [label=ReluBackward0]
	2205711592800 -> 2205711592320
	2205711592800 [label=NativeBatchNormBackward0]
	2205711593232 -> 2205711592800
	2205711593328 -> 2205711592800
	2205639671536 [label="layer3.0.bn_proj.weight
 (512)" fillcolor=lightblue]
	2205639671536 -> 2205711593328
	2205711593328 [label=AccumulateGrad]
	2205711592560 -> 2205711592800
	2205639671616 [label="layer3.0.bn_proj.bias
 (512)" fillcolor=lightblue]
	2205639671616 -> 2205711592560
	2205711592560 [label=AccumulateGrad]
	2205711592368 -> 2205711591792
	2205639741744 [label="layer3.0.projection.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	2205639741744 -> 2205711592368
	2205711592368 [label=AccumulateGrad]
	2205711591696 -> 2205711591552
	2205639741824 [label="layer3.1.block.bn1.weight
 (1024)" fillcolor=lightblue]
	2205639741824 -> 2205711591696
	2205711591696 [label=AccumulateGrad]
	2205711591648 -> 2205711591552
	2205639741904 [label="layer3.1.block.bn1.bias
 (1024)" fillcolor=lightblue]
	2205639741904 -> 2205711591648
	2205711591648 [label=AccumulateGrad]
	2205711591360 -> 2205711591216
	2205639742304 [label="layer3.1.block.conv_x_1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2205639742304 -> 2205711591360
	2205711591360 [label=AccumulateGrad]
	2205711591168 -> 2205711591072
	2205639742384 [label="layer3.1.block.bn2.weight
 (256)" fillcolor=lightblue]
	2205639742384 -> 2205711591168
	2205711591168 [label=AccumulateGrad]
	2205711591120 -> 2205711591072
	2205639742464 [label="layer3.1.block.bn2.bias
 (256)" fillcolor=lightblue]
	2205639742464 -> 2205711591120
	2205711591120 [label=AccumulateGrad]
	2205711590880 -> 2205711590736
	2205639742864 [label="layer3.1.block.conv_x_2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2205639742864 -> 2205711590880
	2205711590880 [label=AccumulateGrad]
	2205711590688 -> 2205711590592
	2205639742944 [label="layer3.1.block.bn3.weight
 (256)" fillcolor=lightblue]
	2205639742944 -> 2205711590688
	2205711590688 [label=AccumulateGrad]
	2205711590640 -> 2205711590592
	2205639743024 [label="layer3.1.block.bn3.bias
 (256)" fillcolor=lightblue]
	2205639743024 -> 2205711590640
	2205711590640 [label=AccumulateGrad]
	2205711590400 -> 2205711590304
	2205639743424 [label="layer3.1.block.conv_x_3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2205639743424 -> 2205711590400
	2205711590400 [label=AccumulateGrad]
	2205711590256 -> 2205711576368
	2205711590160 -> 2205711590016
	2205639743504 [label="layer3.2.block.bn1.weight
 (1024)" fillcolor=lightblue]
	2205639743504 -> 2205711590160
	2205711590160 [label=AccumulateGrad]
	2205711590112 -> 2205711590016
	2205639743584 [label="layer3.2.block.bn1.bias
 (1024)" fillcolor=lightblue]
	2205639743584 -> 2205711590112
	2205711590112 [label=AccumulateGrad]
	2205711589824 -> 2205711589680
	2205639743984 [label="layer3.2.block.conv_x_1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2205639743984 -> 2205711589824
	2205711589824 [label=AccumulateGrad]
	2205711589632 -> 2205711589536
	2205639744064 [label="layer3.2.block.bn2.weight
 (256)" fillcolor=lightblue]
	2205639744064 -> 2205711589632
	2205711589632 [label=AccumulateGrad]
	2205711589584 -> 2205711589536
	2205639744144 [label="layer3.2.block.bn2.bias
 (256)" fillcolor=lightblue]
	2205639744144 -> 2205711589584
	2205711589584 [label=AccumulateGrad]
	2205711576992 -> 2205711576848
	2205639744544 [label="layer3.2.block.conv_x_2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2205639744544 -> 2205711576992
	2205711576992 [label=AccumulateGrad]
	2205711576800 -> 2205711576704
	2205639744624 [label="layer3.2.block.bn3.weight
 (256)" fillcolor=lightblue]
	2205639744624 -> 2205711576800
	2205711576800 [label=AccumulateGrad]
	2205711576752 -> 2205711576704
	2205639744704 [label="layer3.2.block.bn3.bias
 (256)" fillcolor=lightblue]
	2205639744704 -> 2205711576752
	2205711576752 [label=AccumulateGrad]
	2205711576512 -> 2205711576416
	2205639745104 [label="layer3.2.block.conv_x_3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2205639745104 -> 2205711576512
	2205711576512 [label=AccumulateGrad]
	2205711576368 -> 2205711574832
	2205711576272 -> 2205711576128
	2205639745184 [label="layer3.3.block.bn1.weight
 (1024)" fillcolor=lightblue]
	2205639745184 -> 2205711576272
	2205711576272 [label=AccumulateGrad]
	2205711576224 -> 2205711576128
	2205639745264 [label="layer3.3.block.bn1.bias
 (1024)" fillcolor=lightblue]
	2205639745264 -> 2205711576224
	2205711576224 [label=AccumulateGrad]
	2205711575936 -> 2205711575792
	2205639811296 [label="layer3.3.block.conv_x_1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2205639811296 -> 2205711575936
	2205711575936 [label=AccumulateGrad]
	2205711575744 -> 2205711575648
	2205639811376 [label="layer3.3.block.bn2.weight
 (256)" fillcolor=lightblue]
	2205639811376 -> 2205711575744
	2205711575744 [label=AccumulateGrad]
	2205711575696 -> 2205711575648
	2205639811456 [label="layer3.3.block.bn2.bias
 (256)" fillcolor=lightblue]
	2205639811456 -> 2205711575696
	2205711575696 [label=AccumulateGrad]
	2205711575456 -> 2205711575312
	2205639811856 [label="layer3.3.block.conv_x_2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2205639811856 -> 2205711575456
	2205711575456 [label=AccumulateGrad]
	2205711575264 -> 2205711575168
	2205639811936 [label="layer3.3.block.bn3.weight
 (256)" fillcolor=lightblue]
	2205639811936 -> 2205711575264
	2205711575264 [label=AccumulateGrad]
	2205711575216 -> 2205711575168
	2205639812016 [label="layer3.3.block.bn3.bias
 (256)" fillcolor=lightblue]
	2205639812016 -> 2205711575216
	2205711575216 [label=AccumulateGrad]
	2205711574976 -> 2205711574880
	2205639812416 [label="layer3.3.block.conv_x_3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2205639812416 -> 2205711574976
	2205711574976 [label=AccumulateGrad]
	2205711574832 -> 2205711573296
	2205711574736 -> 2205711574592
	2205639812496 [label="layer3.4.block.bn1.weight
 (1024)" fillcolor=lightblue]
	2205639812496 -> 2205711574736
	2205711574736 [label=AccumulateGrad]
	2205711574688 -> 2205711574592
	2205639812576 [label="layer3.4.block.bn1.bias
 (1024)" fillcolor=lightblue]
	2205639812576 -> 2205711574688
	2205711574688 [label=AccumulateGrad]
	2205711574400 -> 2205711574256
	2205639812976 [label="layer3.4.block.conv_x_1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2205639812976 -> 2205711574400
	2205711574400 [label=AccumulateGrad]
	2205711574208 -> 2205711574112
	2205639813056 [label="layer3.4.block.bn2.weight
 (256)" fillcolor=lightblue]
	2205639813056 -> 2205711574208
	2205711574208 [label=AccumulateGrad]
	2205711574160 -> 2205711574112
	2205639813136 [label="layer3.4.block.bn2.bias
 (256)" fillcolor=lightblue]
	2205639813136 -> 2205711574160
	2205711574160 [label=AccumulateGrad]
	2205711573920 -> 2205711573776
	2205639813536 [label="layer3.4.block.conv_x_2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2205639813536 -> 2205711573920
	2205711573920 [label=AccumulateGrad]
	2205711573728 -> 2205711573632
	2205639813616 [label="layer3.4.block.bn3.weight
 (256)" fillcolor=lightblue]
	2205639813616 -> 2205711573728
	2205711573728 [label=AccumulateGrad]
	2205711573680 -> 2205711573632
	2205639813696 [label="layer3.4.block.bn3.bias
 (256)" fillcolor=lightblue]
	2205639813696 -> 2205711573680
	2205711573680 [label=AccumulateGrad]
	2205711573440 -> 2205711573344
	2205639814096 [label="layer3.4.block.conv_x_3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2205639814096 -> 2205711573440
	2205711573440 [label=AccumulateGrad]
	2205711573296 -> 2205711563504
	2205711573200 -> 2205711564752
	2205639814176 [label="layer3.5.block.bn1.weight
 (1024)" fillcolor=lightblue]
	2205639814176 -> 2205711573200
	2205711573200 [label=AccumulateGrad]
	2205711573152 -> 2205711564752
	2205639814256 [label="layer3.5.block.bn1.bias
 (1024)" fillcolor=lightblue]
	2205639814256 -> 2205711573152
	2205711573152 [label=AccumulateGrad]
	2205711564608 -> 2205711564464
	2205639814656 [label="layer3.5.block.conv_x_1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2205639814656 -> 2205711564608
	2205711564608 [label=AccumulateGrad]
	2205711564416 -> 2205711564320
	2205639814736 [label="layer3.5.block.bn2.weight
 (256)" fillcolor=lightblue]
	2205639814736 -> 2205711564416
	2205711564416 [label=AccumulateGrad]
	2205711564368 -> 2205711564320
	2205639814816 [label="layer3.5.block.bn2.bias
 (256)" fillcolor=lightblue]
	2205639814816 -> 2205711564368
	2205711564368 [label=AccumulateGrad]
	2205711564128 -> 2205711563984
	2205639872656 [label="layer3.5.block.conv_x_2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2205639872656 -> 2205711564128
	2205711564128 [label=AccumulateGrad]
	2205711563936 -> 2205711563840
	2205639872736 [label="layer3.5.block.bn3.weight
 (256)" fillcolor=lightblue]
	2205639872736 -> 2205711563936
	2205711563936 [label=AccumulateGrad]
	2205711563888 -> 2205711563840
	2205639872816 [label="layer3.5.block.bn3.bias
 (256)" fillcolor=lightblue]
	2205639872816 -> 2205711563888
	2205711563888 [label=AccumulateGrad]
	2205711563648 -> 2205711563552
	2205639873216 [label="layer3.5.block.conv_x_3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2205639873216 -> 2205711563648
	2205711563648 [label=AccumulateGrad]
	2205711563504 -> 2205711563360
	2205711563312 -> 2205711563216
	2205639873296 [label="layer4.0.block.bn1.weight
 (1024)" fillcolor=lightblue]
	2205639873296 -> 2205711563312
	2205711563312 [label=AccumulateGrad]
	2205711563264 -> 2205711563216
	2205639873376 [label="layer4.0.block.bn1.bias
 (1024)" fillcolor=lightblue]
	2205639873376 -> 2205711563264
	2205711563264 [label=AccumulateGrad]
	2205711563024 -> 2205711562880
	2205639873776 [label="layer4.0.block.conv_x_1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	2205639873776 -> 2205711563024
	2205711563024 [label=AccumulateGrad]
	2205711562832 -> 2205711562736
	2205639873856 [label="layer4.0.block.bn2.weight
 (512)" fillcolor=lightblue]
	2205639873856 -> 2205711562832
	2205711562832 [label=AccumulateGrad]
	2205711562784 -> 2205711562736
	2205639873936 [label="layer4.0.block.bn2.bias
 (512)" fillcolor=lightblue]
	2205639873936 -> 2205711562784
	2205711562784 [label=AccumulateGrad]
	2205711562544 -> 2205711562400
	2205639874336 [label="layer4.0.block.conv_x_2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2205639874336 -> 2205711562544
	2205711562544 [label=AccumulateGrad]
	2205711562352 -> 2205711562256
	2205639874416 [label="layer4.0.block.bn3.weight
 (512)" fillcolor=lightblue]
	2205639874416 -> 2205711562352
	2205711562352 [label=AccumulateGrad]
	2205711562304 -> 2205711562256
	2205639874496 [label="layer4.0.block.bn3.bias
 (512)" fillcolor=lightblue]
	2205639874496 -> 2205711562304
	2205711562304 [label=AccumulateGrad]
	2205711562064 -> 2205711561968
	2205639874896 [label="layer4.0.block.conv_x_3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2205639874896 -> 2205711562064
	2205711562064 [label=AccumulateGrad]
	2205711561920 -> 2205711478448
	2205711561920 [label=ConvolutionBackward0]
	2205711562448 -> 2205711561920
	2205711562448 [label=ReluBackward0]
	2205711562928 -> 2205711562448
	2205711562928 [label=NativeBatchNormBackward0]
	2205711563360 -> 2205711562928
	2205711563456 -> 2205711562928
	2205639874976 [label="layer4.0.bn_proj.weight
 (1024)" fillcolor=lightblue]
	2205639874976 -> 2205711563456
	2205711563456 [label=AccumulateGrad]
	2205711562688 -> 2205711562928
	2205639875056 [label="layer4.0.bn_proj.bias
 (1024)" fillcolor=lightblue]
	2205639875056 -> 2205711562688
	2205711562688 [label=AccumulateGrad]
	2205711562496 -> 2205711561920
	2205639875456 [label="layer4.0.projection.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	2205639875456 -> 2205711562496
	2205711562496 [label=AccumulateGrad]
	2205711561824 -> 2205711561680
	2205639875536 [label="layer4.1.block.bn1.weight
 (2048)" fillcolor=lightblue]
	2205639875536 -> 2205711561824
	2205711561824 [label=AccumulateGrad]
	2205711561776 -> 2205711561680
	2205639875616 [label="layer4.1.block.bn1.bias
 (2048)" fillcolor=lightblue]
	2205639875616 -> 2205711561776
	2205711561776 [label=AccumulateGrad]
	2205711561488 -> 2205711561344
	2205639876016 [label="layer4.1.block.conv_x_1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2205639876016 -> 2205711561488
	2205711561488 [label=AccumulateGrad]
	2205711561296 -> 2205711561200
	2205639876096 [label="layer4.1.block.bn2.weight
 (512)" fillcolor=lightblue]
	2205639876096 -> 2205711561296
	2205711561296 [label=AccumulateGrad]
	2205711561248 -> 2205711561200
	2205639876176 [label="layer4.1.block.bn2.bias
 (512)" fillcolor=lightblue]
	2205639876176 -> 2205711561248
	2205711561248 [label=AccumulateGrad]
	2205711561008 -> 2205711560864
	2205639942208 [label="layer4.1.block.conv_x_2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2205639942208 -> 2205711561008
	2205711561008 [label=AccumulateGrad]
	2205711560816 -> 2205711478736
	2205639942288 [label="layer4.1.block.bn3.weight
 (512)" fillcolor=lightblue]
	2205639942288 -> 2205711560816
	2205711560816 [label=AccumulateGrad]
	2205711560768 -> 2205711478736
	2205639942368 [label="layer4.1.block.bn3.bias
 (512)" fillcolor=lightblue]
	2205639942368 -> 2205711560768
	2205711560768 [label=AccumulateGrad]
	2205711478592 -> 2205711478496
	2205639942768 [label="layer4.1.block.conv_x_3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2205639942768 -> 2205711478592
	2205711478592 [label=AccumulateGrad]
	2205711478448 -> 2205711477056
	2205711478352 -> 2205711478256
	2205639942848 [label="layer4.2.block.bn1.weight
 (2048)" fillcolor=lightblue]
	2205639942848 -> 2205711478352
	2205711478352 [label=AccumulateGrad]
	2205711478304 -> 2205711478256
	2205639942928 [label="layer4.2.block.bn1.bias
 (2048)" fillcolor=lightblue]
	2205639942928 -> 2205711478304
	2205711478304 [label=AccumulateGrad]
	2205711478064 -> 2205711477920
	2205639943328 [label="layer4.2.block.conv_x_1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2205639943328 -> 2205711478064
	2205711478064 [label=AccumulateGrad]
	2205711477872 -> 2205711477824
	2205639943408 [label="layer4.2.block.bn2.weight
 (512)" fillcolor=lightblue]
	2205639943408 -> 2205711477872
	2205711477872 [label=AccumulateGrad]
	2205711477728 -> 2205711477824
	2205639943488 [label="layer4.2.block.bn2.bias
 (512)" fillcolor=lightblue]
	2205639943488 -> 2205711477728
	2205711477728 [label=AccumulateGrad]
	2205711477632 -> 2205711477488
	2205639943888 [label="layer4.2.block.conv_x_2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2205639943888 -> 2205711477632
	2205711477632 [label=AccumulateGrad]
	2205711477440 -> 2205711477392
	2205639943968 [label="layer4.2.block.bn3.weight
 (512)" fillcolor=lightblue]
	2205639943968 -> 2205711477440
	2205711477440 [label=AccumulateGrad]
	2205711477296 -> 2205711477392
	2205639944048 [label="layer4.2.block.bn3.bias
 (512)" fillcolor=lightblue]
	2205639944048 -> 2205711477296
	2205711477296 [label=AccumulateGrad]
	2205711477200 -> 2205711477104
	2205639944448 [label="layer4.2.block.conv_x_3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2205639944448 -> 2205711477200
	2205711477200 [label=AccumulateGrad]
	2205711477056 -> 2205711476912
	2205711476864 -> 2205711476816
	2205639944528 [label="post_bn.weight
 (2048)" fillcolor=lightblue]
	2205639944528 -> 2205711476864
	2205711476864 [label=AccumulateGrad]
	2205711476432 -> 2205711476816
	2205639944608 [label="post_bn.bias
 (2048)" fillcolor=lightblue]
	2205639944608 -> 2205711476432
	2205711476432 [label=AccumulateGrad]
	2205711476336 -> 2205711476096
	2205711476336 [label=TBackward0]
	2205711476672 -> 2205711476336
	2205639944928 [label="linear.weight
 (1000, 2048)" fillcolor=lightblue]
	2205639944928 -> 2205711476672
	2205711476672 [label=AccumulateGrad]
	2205711476240 -> 2205711476192
	2205639945008 [label="linear.bias
 (1000)" fillcolor=lightblue]
	2205639945008 -> 2205711476240
	2205711476240 [label=AccumulateGrad]
	2205711476192 -> 2205711506704
}
