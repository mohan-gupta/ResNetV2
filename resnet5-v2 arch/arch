digraph {
	graph [size="154.65,154.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2465059342752 [label="
 (1000)" fillcolor=darkolivegreen1]
	2463203371568 [label=AddBackward0]
	2463203370944 -> 2463203371568
	2463203370944 [label=SqueezeBackward3]
	2463203371904 -> 2463203370944
	2463203371904 [label=MmBackward0]
	2463203372144 -> 2463203371904
	2463203372144 [label=UnsqueezeBackward0]
	2463203371376 -> 2463203372144
	2463203371376 [label=ViewBackward0]
	2463203371952 -> 2463203371376
	2463203371952 [label=MeanBackward1]
	2463203370320 -> 2463203371952
	2463203370320 [label=ReluBackward0]
	2463203371040 -> 2463203370320
	2463203371040 [label=NativeBatchNormBackward0]
	2463203370560 -> 2463203371040
	2463203370560 [label=AddBackward0]
	2463203372096 -> 2463203370560
	2463203372096 [label=ConvolutionBackward0]
	2463203372528 -> 2463203372096
	2463203372528 [label=ReluBackward0]
	2463198531744 -> 2463203372528
	2463198531744 [label=NativeBatchNormBackward0]
	2463202156896 -> 2463198531744
	2463202156896 [label=ConvolutionBackward0]
	2463202158240 -> 2463202156896
	2463202158240 [label=ReluBackward0]
	2463202158672 -> 2463202158240
	2463202158672 [label=NativeBatchNormBackward0]
	2463202158336 -> 2463202158672
	2463202158336 [label=ConvolutionBackward0]
	2463202158912 -> 2463202158336
	2463202158912 [label=ReluBackward0]
	2463202157760 -> 2463202158912
	2463202157760 [label=NativeBatchNormBackward0]
	2463203371424 -> 2463202157760
	2463203371424 [label=AddBackward0]
	2463199554912 -> 2463203371424
	2463199554912 [label=ConvolutionBackward0]
	2463199552176 -> 2463199554912
	2463199552176 [label=ReluBackward0]
	2463199554720 -> 2463199552176
	2463199554720 [label=NativeBatchNormBackward0]
	2463199554384 -> 2463199554720
	2463199554384 [label=ConvolutionBackward0]
	2463199552128 -> 2463199554384
	2463199552128 [label=ReluBackward0]
	2463199552992 -> 2463199552128
	2463199552992 [label=NativeBatchNormBackward0]
	2463199554144 -> 2463199552992
	2463199554144 [label=ConvolutionBackward0]
	2463199555392 -> 2463199554144
	2463199555392 [label=ReluBackward0]
	2463199555344 -> 2463199555392
	2463199555344 [label=NativeBatchNormBackward0]
	2463199552368 -> 2463199555344
	2463199552368 [label=AddBackward0]
	2463199555440 -> 2463199552368
	2463199555440 [label=ConvolutionBackward0]
	2463199554240 -> 2463199555440
	2463199554240 [label=ReluBackward0]
	2463224171392 -> 2463199554240
	2463224171392 [label=NativeBatchNormBackward0]
	2463222525136 -> 2463224171392
	2463222525136 [label=ConvolutionBackward0]
	2463199273120 -> 2463222525136
	2463199273120 [label=ReluBackward0]
	2463199273072 -> 2463199273120
	2463199273072 [label=NativeBatchNormBackward0]
	2463199273024 -> 2463199273072
	2463199273024 [label=ConvolutionBackward0]
	2463224212496 -> 2463199273024
	2463224212496 [label=ReluBackward0]
	2463224180400 -> 2463224212496
	2463224180400 [label=NativeBatchNormBackward0]
	2463199174912 -> 2463224180400
	2463199174912 [label=AddBackward0]
	2464759190480 -> 2463199174912
	2464759190480 [label=ConvolutionBackward0]
	2464759192112 -> 2464759190480
	2464759192112 [label=ReluBackward0]
	2464759200352 -> 2464759192112
	2464759200352 [label=NativeBatchNormBackward0]
	2464759200304 -> 2464759200352
	2464759200304 [label=ConvolutionBackward0]
	2464759201600 -> 2464759200304
	2464759201600 [label=ReluBackward0]
	2464759199968 -> 2464759201600
	2464759199968 [label=NativeBatchNormBackward0]
	2464759200688 -> 2464759199968
	2464759200688 [label=ConvolutionBackward0]
	2464759200160 -> 2464759200688
	2464759200160 [label=ReluBackward0]
	2464759198576 -> 2464759200160
	2464759198576 [label=NativeBatchNormBackward0]
	2464759191056 -> 2464759198576
	2464759191056 [label=AddBackward0]
	2464759198240 -> 2464759191056
	2464759198240 [label=ConvolutionBackward0]
	2464759201312 -> 2464759198240
	2464759201312 [label=ReluBackward0]
	2464759201168 -> 2464759201312
	2464759201168 [label=NativeBatchNormBackward0]
	2464759201504 -> 2464759201168
	2464759201504 [label=ConvolutionBackward0]
	2464759199632 -> 2464759201504
	2464759199632 [label=ReluBackward0]
	2464759198720 -> 2464759199632
	2464759198720 [label=NativeBatchNormBackward0]
	2464759199440 -> 2464759198720
	2464759199440 [label=ConvolutionBackward0]
	2464759199392 -> 2464759199440
	2464759199392 [label=ReluBackward0]
	2464759199488 -> 2464759199392
	2464759199488 [label=NativeBatchNormBackward0]
	2464759198048 -> 2464759199488
	2464759198048 [label=AddBackward0]
	2463202167776 -> 2464759198048
	2463202167776 [label=ConvolutionBackward0]
	2463202168160 -> 2463202167776
	2463202168160 [label=ReluBackward0]
	2463202165472 -> 2463202168160
	2463202165472 [label=NativeBatchNormBackward0]
	2463202167968 -> 2463202165472
	2463202167968 [label=ConvolutionBackward0]
	2463202168496 -> 2463202167968
	2463202168496 [label=ReluBackward0]
	2463202168448 -> 2463202168496
	2463202168448 [label=NativeBatchNormBackward0]
	2463202168016 -> 2463202168448
	2463202168016 [label=ConvolutionBackward0]
	2463202168592 -> 2463202168016
	2463202168592 [label=ReluBackward0]
	2463202168064 -> 2463202168592
	2463202168064 [label=NativeBatchNormBackward0]
	2463202165616 -> 2463202168064
	2463202165616 [label=AddBackward0]
	2463220371808 -> 2463202165616
	2463220371808 [label=ConvolutionBackward0]
	2463220371952 -> 2463220371808
	2463220371952 [label=ReluBackward0]
	2463220372096 -> 2463220371952
	2463220372096 [label=NativeBatchNormBackward0]
	2463220372192 -> 2463220372096
	2463220372192 [label=ConvolutionBackward0]
	2463220372384 -> 2463220372192
	2463220372384 [label=ReluBackward0]
	2463220372528 -> 2463220372384
	2463220372528 [label=NativeBatchNormBackward0]
	2463220372624 -> 2463220372528
	2463220372624 [label=ConvolutionBackward0]
	2463220372816 -> 2463220372624
	2463220372816 [label=ReluBackward0]
	2463220372960 -> 2463220372816
	2463220372960 [label=NativeBatchNormBackward0]
	2463220371760 -> 2463220372960
	2463220371760 [label=AddBackward0]
	2463220373200 -> 2463220371760
	2463220373200 [label=ConvolutionBackward0]
	2463220373344 -> 2463220373200
	2463220373344 [label=ReluBackward0]
	2463220373488 -> 2463220373344
	2463220373488 [label=NativeBatchNormBackward0]
	2463220373584 -> 2463220373488
	2463220373584 [label=ConvolutionBackward0]
	2463220373776 -> 2463220373584
	2463220373776 [label=ReluBackward0]
	2463220373920 -> 2463220373776
	2463220373920 [label=NativeBatchNormBackward0]
	2463220374016 -> 2463220373920
	2463220374016 [label=ConvolutionBackward0]
	2463220374208 -> 2463220374016
	2463220374208 [label=ReluBackward0]
	2463220374352 -> 2463220374208
	2463220374352 [label=NativeBatchNormBackward0]
	2463220373152 -> 2463220374352
	2463220373152 [label=AddBackward0]
	2463220374592 -> 2463220373152
	2463220374592 [label=ConvolutionBackward0]
	2463220374736 -> 2463220374592
	2463220374736 [label=ReluBackward0]
	2463220374880 -> 2463220374736
	2463220374880 [label=NativeBatchNormBackward0]
	2463220374976 -> 2463220374880
	2463220374976 [label=ConvolutionBackward0]
	2463220375168 -> 2463220374976
	2463220375168 [label=ReluBackward0]
	2463220375312 -> 2463220375168
	2463220375312 [label=NativeBatchNormBackward0]
	2463220375408 -> 2463220375312
	2463220375408 [label=ConvolutionBackward0]
	2463220375504 -> 2463220375408
	2463220375504 [label=ReluBackward0]
	2463220355328 -> 2463220375504
	2463220355328 [label=NativeBatchNormBackward0]
	2463220355424 -> 2463220355328
	2463220355424 [label=AddBackward0]
	2463220355616 -> 2463220355424
	2463220355616 [label=ConvolutionBackward0]
	2463220355760 -> 2463220355616
	2463220355760 [label=ReluBackward0]
	2463220355904 -> 2463220355760
	2463220355904 [label=NativeBatchNormBackward0]
	2463220356000 -> 2463220355904
	2463220356000 [label=ConvolutionBackward0]
	2463220356192 -> 2463220356000
	2463220356192 [label=ReluBackward0]
	2463220356336 -> 2463220356192
	2463220356336 [label=NativeBatchNormBackward0]
	2463220356432 -> 2463220356336
	2463220356432 [label=ConvolutionBackward0]
	2463220356624 -> 2463220356432
	2463220356624 [label=ReluBackward0]
	2463220356768 -> 2463220356624
	2463220356768 [label=NativeBatchNormBackward0]
	2463220355568 -> 2463220356768
	2463220355568 [label=AddBackward0]
	2463220357008 -> 2463220355568
	2463220357008 [label=ConvolutionBackward0]
	2463220357152 -> 2463220357008
	2463220357152 [label=ReluBackward0]
	2463220357296 -> 2463220357152
	2463220357296 [label=NativeBatchNormBackward0]
	2463220357392 -> 2463220357296
	2463220357392 [label=ConvolutionBackward0]
	2463220357584 -> 2463220357392
	2463220357584 [label=ReluBackward0]
	2463220357728 -> 2463220357584
	2463220357728 [label=NativeBatchNormBackward0]
	2463220357824 -> 2463220357728
	2463220357824 [label=ConvolutionBackward0]
	2463220358016 -> 2463220357824
	2463220358016 [label=ReluBackward0]
	2463220358160 -> 2463220358016
	2463220358160 [label=NativeBatchNormBackward0]
	2463220356960 -> 2463220358160
	2463220356960 [label=AddBackward0]
	2463220358400 -> 2463220356960
	2463220358400 [label=ConvolutionBackward0]
	2463220358544 -> 2463220358400
	2463220358544 [label=ReluBackward0]
	2463220358688 -> 2463220358544
	2463220358688 [label=NativeBatchNormBackward0]
	2463220358784 -> 2463220358688
	2463220358784 [label=ConvolutionBackward0]
	2463220358976 -> 2463220358784
	2463220358976 [label=ReluBackward0]
	2463220359120 -> 2463220358976
	2463220359120 [label=NativeBatchNormBackward0]
	2463220359024 -> 2463220359120
	2463220359024 [label=ConvolutionBackward0]
	2463220425008 -> 2463220359024
	2463220425008 [label=ReluBackward0]
	2463220425152 -> 2463220425008
	2463220425152 [label=NativeBatchNormBackward0]
	2463220358352 -> 2463220425152
	2463220358352 [label=AddBackward0]
	2463220425392 -> 2463220358352
	2463220425392 [label=ConvolutionBackward0]
	2463203370896 -> 2463220425392
	2463203370896 [label=ReluBackward0]
	2463203369936 -> 2463203370896
	2463203369936 [label=NativeBatchNormBackward0]
	2463203370704 -> 2463203369936
	2463203370704 [label=ConvolutionBackward0]
	2463203370512 -> 2463203370704
	2463203370512 [label=ReluBackward0]
	2463203369888 -> 2463203370512
	2463203369888 [label=NativeBatchNormBackward0]
	2463201987408 -> 2463203369888
	2463201987408 [label=ConvolutionBackward0]
	2463201987888 -> 2463201987408
	2463201987888 [label=ReluBackward0]
	2463201987456 -> 2463201987888
	2463201987456 [label=NativeBatchNormBackward0]
	2463201987120 -> 2463201987456
	2463201987120 [label=AddBackward0]
	2463201987024 -> 2463201987120
	2463201987024 [label=ConvolutionBackward0]
	2463201986016 -> 2463201987024
	2463201986016 [label=ReluBackward0]
	2463201986496 -> 2463201986016
	2463201986496 [label=NativeBatchNormBackward0]
	2463199467408 -> 2463201986496
	2463199467408 [label=ConvolutionBackward0]
	2463199468368 -> 2463199467408
	2463199468368 [label=ReluBackward0]
	2463199468272 -> 2463199468368
	2463199468272 [label=NativeBatchNormBackward0]
	2463199466064 -> 2463199468272
	2463199466064 [label=ConvolutionBackward0]
	2463220425680 -> 2463199466064
	2463220425680 [label=ReluBackward0]
	2463220425824 -> 2463220425680
	2463220425824 [label=NativeBatchNormBackward0]
	2463201987072 -> 2463220425824
	2463201987072 [label=AddBackward0]
	2463220426064 -> 2463201987072
	2463220426064 [label=ConvolutionBackward0]
	2463220426208 -> 2463220426064
	2463220426208 [label=ReluBackward0]
	2463220426352 -> 2463220426208
	2463220426352 [label=NativeBatchNormBackward0]
	2463220426448 -> 2463220426352
	2463220426448 [label=ConvolutionBackward0]
	2463220426640 -> 2463220426448
	2463220426640 [label=ReluBackward0]
	2463220426784 -> 2463220426640
	2463220426784 [label=NativeBatchNormBackward0]
	2463220426880 -> 2463220426784
	2463220426880 [label=ConvolutionBackward0]
	2463220427072 -> 2463220426880
	2463220427072 [label=ReluBackward0]
	2463220427216 -> 2463220427072
	2463220427216 [label=NativeBatchNormBackward0]
	2463220426016 -> 2463220427216
	2463220426016 [label=AddBackward0]
	2463220427456 -> 2463220426016
	2463220427456 [label=ConvolutionBackward0]
	2463220427600 -> 2463220427456
	2463220427600 [label=ReluBackward0]
	2463220427744 -> 2463220427600
	2463220427744 [label=NativeBatchNormBackward0]
	2463220427840 -> 2463220427744
	2463220427840 [label=ConvolutionBackward0]
	2463220428032 -> 2463220427840
	2463220428032 [label=ReluBackward0]
	2463220428176 -> 2463220428032
	2463220428176 [label=NativeBatchNormBackward0]
	2463220428272 -> 2463220428176
	2463220428272 [label=ConvolutionBackward0]
	2463220428464 -> 2463220428272
	2463220428464 [label=ReluBackward0]
	2463220428608 -> 2463220428464
	2463220428608 [label=NativeBatchNormBackward0]
	2463220428704 -> 2463220428608
	2463220428704 [label=MaxPool2DWithIndicesBackward0]
	2463202255008 -> 2463220428704
	2463202255008 [label=ConvolutionBackward0]
	2463202255104 -> 2463202255008
	2464961015696 [label="conv_1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	2464961015696 -> 2463202255104
	2463202255104 [label=AccumulateGrad]
	2463202255056 -> 2463202255008
	2464961015456 [label="conv_1.bias
 (64)" fillcolor=lightblue]
	2464961015456 -> 2463202255056
	2463202255056 [label=AccumulateGrad]
	2463220428656 -> 2463220428608
	2464961014736 [label="layer1.0.bottleneck.bn1.weight
 (64)" fillcolor=lightblue]
	2464961014736 -> 2463220428656
	2463220428656 [label=AccumulateGrad]
	2463220428512 -> 2463220428608
	2464961077472 [label="layer1.0.bottleneck.bn1.bias
 (64)" fillcolor=lightblue]
	2464961077472 -> 2463220428512
	2463220428512 [label=AccumulateGrad]
	2463220428416 -> 2463220428272
	2464961078112 [label="layer1.0.bottleneck.conv_x_1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2464961078112 -> 2463220428416
	2463220428416 [label=AccumulateGrad]
	2463220428224 -> 2463220428176
	2464961078192 [label="layer1.0.bottleneck.bn2.weight
 (64)" fillcolor=lightblue]
	2464961078192 -> 2463220428224
	2463220428224 [label=AccumulateGrad]
	2463220428080 -> 2463220428176
	2464961078272 [label="layer1.0.bottleneck.bn2.bias
 (64)" fillcolor=lightblue]
	2464961078272 -> 2463220428080
	2463220428080 [label=AccumulateGrad]
	2463220427984 -> 2463220427840
	2464961078672 [label="layer1.0.bottleneck.conv_x_2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2464961078672 -> 2463220427984
	2463220427984 [label=AccumulateGrad]
	2463220427792 -> 2463220427744
	2464961078752 [label="layer1.0.bottleneck.bn3.weight
 (64)" fillcolor=lightblue]
	2464961078752 -> 2463220427792
	2463220427792 [label=AccumulateGrad]
	2463220427648 -> 2463220427744
	2464961078832 [label="layer1.0.bottleneck.bn3.bias
 (64)" fillcolor=lightblue]
	2464961078832 -> 2463220427648
	2463220427648 [label=AccumulateGrad]
	2463220427552 -> 2463220427456
	2464961079152 [label="layer1.0.bottleneck.conv_x_3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2464961079152 -> 2463220427552
	2463220427552 [label=AccumulateGrad]
	2463220427408 -> 2463220426016
	2463220427408 [label=ConvolutionBackward0]
	2463220427888 -> 2463220427408
	2463220427888 [label=ReluBackward0]
	2463220428320 -> 2463220427888
	2463220428320 [label=NativeBatchNormBackward0]
	2463220428704 -> 2463220428320
	2463220428752 -> 2463220428320
	2464961040768 [label="layer1.0.bn_proj.weight
 (64)" fillcolor=lightblue]
	2464961040768 -> 2463220428752
	2463220428752 [label=AccumulateGrad]
	2463220428128 -> 2463220428320
	2464961040528 [label="layer1.0.bn_proj.bias
 (64)" fillcolor=lightblue]
	2464961040528 -> 2463220428128
	2463220428128 [label=AccumulateGrad]
	2463220427936 -> 2463220427408
	2464961079072 [label="layer1.0.projection.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2464961079072 -> 2463220427936
	2463220427936 [label=AccumulateGrad]
	2463220427312 -> 2463220427216
	2464961079312 [label="layer1.1.bottleneck.bn1.weight
 (256)" fillcolor=lightblue]
	2464961079312 -> 2463220427312
	2463220427312 [label=AccumulateGrad]
	2463220427264 -> 2463220427216
	2464961079392 [label="layer1.1.bottleneck.bn1.bias
 (256)" fillcolor=lightblue]
	2464961079392 -> 2463220427264
	2463220427264 [label=AccumulateGrad]
	2463220427024 -> 2463220426880
	2464961079792 [label="layer1.1.bottleneck.conv_x_1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2464961079792 -> 2463220427024
	2463220427024 [label=AccumulateGrad]
	2463220426832 -> 2463220426784
	2464961079872 [label="layer1.1.bottleneck.bn2.weight
 (64)" fillcolor=lightblue]
	2464961079872 -> 2463220426832
	2463220426832 [label=AccumulateGrad]
	2463220426688 -> 2463220426784
	2464961079952 [label="layer1.1.bottleneck.bn2.bias
 (64)" fillcolor=lightblue]
	2464961079952 -> 2463220426688
	2463220426688 [label=AccumulateGrad]
	2463220426592 -> 2463220426448
	2464961080272 [label="layer1.1.bottleneck.conv_x_2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2464961080272 -> 2463220426592
	2463220426592 [label=AccumulateGrad]
	2463220426400 -> 2463220426352
	2464961080352 [label="layer1.1.bottleneck.bn3.weight
 (64)" fillcolor=lightblue]
	2464961080352 -> 2463220426400
	2463220426400 [label=AccumulateGrad]
	2463220426256 -> 2463220426352
	2464961080432 [label="layer1.1.bottleneck.bn3.bias
 (64)" fillcolor=lightblue]
	2464961080432 -> 2463220426256
	2463220426256 [label=AccumulateGrad]
	2463220426160 -> 2463220426064
	2464961080832 [label="layer1.1.bottleneck.conv_x_3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2464961080832 -> 2463220426160
	2463220426160 [label=AccumulateGrad]
	2463220426016 -> 2463201987072
	2463220425920 -> 2463220425824
	2464961080912 [label="layer1.2.bottleneck.bn1.weight
 (256)" fillcolor=lightblue]
	2464961080912 -> 2463220425920
	2463220425920 [label=AccumulateGrad]
	2463220425872 -> 2463220425824
	2464961080992 [label="layer1.2.bottleneck.bn1.bias
 (256)" fillcolor=lightblue]
	2464961080992 -> 2463220425872
	2463220425872 [label=AccumulateGrad]
	2463220425632 -> 2463199466064
	2464961171600 [label="layer1.2.bottleneck.conv_x_1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2464961171600 -> 2463220425632
	2463220425632 [label=AccumulateGrad]
	2463220425488 -> 2463199468272
	2464961171680 [label="layer1.2.bottleneck.bn2.weight
 (64)" fillcolor=lightblue]
	2464961171680 -> 2463220425488
	2463220425488 [label=AccumulateGrad]
	2463220425536 -> 2463199468272
	2464961171760 [label="layer1.2.bottleneck.bn2.bias
 (64)" fillcolor=lightblue]
	2464961171760 -> 2463220425536
	2463220425536 [label=AccumulateGrad]
	2463199467360 -> 2463199467408
	2464961172160 [label="layer1.2.bottleneck.conv_x_2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2464961172160 -> 2463199467360
	2463199467360 [label=AccumulateGrad]
	2463199467120 -> 2463201986496
	2464961172240 [label="layer1.2.bottleneck.bn3.weight
 (64)" fillcolor=lightblue]
	2464961172240 -> 2463199467120
	2463199467120 [label=AccumulateGrad]
	2463199466256 -> 2463201986496
	2464961172320 [label="layer1.2.bottleneck.bn3.bias
 (64)" fillcolor=lightblue]
	2464961172320 -> 2463199466256
	2463199466256 [label=AccumulateGrad]
	2463201986832 -> 2463201987024
	2464961172720 [label="layer1.2.bottleneck.conv_x_3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2464961172720 -> 2463201986832
	2463201986832 [label=AccumulateGrad]
	2463201987072 -> 2463201987120
	2463201987168 -> 2463201987456
	2464961172640 [label="layer2.0.bottleneck.bn1.weight
 (256)" fillcolor=lightblue]
	2464961172640 -> 2463201987168
	2463201987168 [label=AccumulateGrad]
	2463201987648 -> 2463201987456
	2464961172800 [label="layer2.0.bottleneck.bn1.bias
 (256)" fillcolor=lightblue]
	2464961172800 -> 2463201987648
	2463201987648 [label=AccumulateGrad]
	2463201987312 -> 2463201987408
	2464961173200 [label="layer2.0.bottleneck.conv_x_1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	2464961173200 -> 2463201987312
	2463201987312 [label=AccumulateGrad]
	2463201988032 -> 2463203369888
	2464961173280 [label="layer2.0.bottleneck.bn2.weight
 (128)" fillcolor=lightblue]
	2464961173280 -> 2463201988032
	2463201988032 [label=AccumulateGrad]
	2463201988512 -> 2463203369888
	2464961173360 [label="layer2.0.bottleneck.bn2.bias
 (128)" fillcolor=lightblue]
	2464961173360 -> 2463201988512
	2463201988512 [label=AccumulateGrad]
	2463203370464 -> 2463203370704
	2464961173760 [label="layer2.0.bottleneck.conv_x_2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2464961173760 -> 2463203370464
	2463203370464 [label=AccumulateGrad]
	2463203370272 -> 2463203369936
	2464961173840 [label="layer2.0.bottleneck.bn3.weight
 (128)" fillcolor=lightblue]
	2464961173840 -> 2463203370272
	2463203370272 [label=AccumulateGrad]
	2463203370752 -> 2463203369936
	2464961173920 [label="layer2.0.bottleneck.bn3.bias
 (128)" fillcolor=lightblue]
	2464961173920 -> 2463203370752
	2463203370752 [label=AccumulateGrad]
	2463203370080 -> 2463220425392
	2464961174320 [label="layer2.0.bottleneck.conv_x_3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2464961174320 -> 2463203370080
	2463203370080 [label=AccumulateGrad]
	2463220425344 -> 2463220358352
	2463220425344 [label=ConvolutionBackward0]
	2463203370416 -> 2463220425344
	2463203370416 [label=ReluBackward0]
	2463203369024 -> 2463203370416
	2463203369024 [label=NativeBatchNormBackward0]
	2463201987120 -> 2463203369024
	2463201986112 -> 2463203369024
	2464961174400 [label="layer2.0.bn_proj.weight
 (256)" fillcolor=lightblue]
	2464961174400 -> 2463201986112
	2463201986112 [label=AccumulateGrad]
	2463201988368 -> 2463203369024
	2464961174480 [label="layer2.0.bn_proj.bias
 (256)" fillcolor=lightblue]
	2464961174480 -> 2463201988368
	2463201988368 [label=AccumulateGrad]
	2463203369840 -> 2463220425344
	2464961174880 [label="layer2.0.projection.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2464961174880 -> 2463203369840
	2463203369840 [label=AccumulateGrad]
	2463220425248 -> 2463220425152
	2464830451600 [label="layer2.1.bottleneck.bn1.weight
 (512)" fillcolor=lightblue]
	2464830451600 -> 2463220425248
	2463220425248 [label=AccumulateGrad]
	2463220425200 -> 2463220425152
	2464961174800 [label="layer2.1.bottleneck.bn1.bias
 (512)" fillcolor=lightblue]
	2464961174800 -> 2463220425200
	2463220425200 [label=AccumulateGrad]
	2463220424960 -> 2463220359024
	2464961175360 [label="layer2.1.bottleneck.conv_x_1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2464961175360 -> 2463220424960
	2463220424960 [label=AccumulateGrad]
	2463220424816 -> 2463220359120
	2464961175440 [label="layer2.1.bottleneck.bn2.weight
 (128)" fillcolor=lightblue]
	2464961175440 -> 2463220424816
	2463220424816 [label=AccumulateGrad]
	2463220424768 -> 2463220359120
	2464961232960 [label="layer2.1.bottleneck.bn2.bias
 (128)" fillcolor=lightblue]
	2464961232960 -> 2463220424768
	2463220424768 [label=AccumulateGrad]
	2463220358928 -> 2463220358784
	2464961233360 [label="layer2.1.bottleneck.conv_x_2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2464961233360 -> 2463220358928
	2463220358928 [label=AccumulateGrad]
	2463220358736 -> 2463220358688
	2464961233440 [label="layer2.1.bottleneck.bn3.weight
 (128)" fillcolor=lightblue]
	2464961233440 -> 2463220358736
	2463220358736 [label=AccumulateGrad]
	2463220358592 -> 2463220358688
	2464961233520 [label="layer2.1.bottleneck.bn3.bias
 (128)" fillcolor=lightblue]
	2464961233520 -> 2463220358592
	2463220358592 [label=AccumulateGrad]
	2463220358496 -> 2463220358400
	2464961233920 [label="layer2.1.bottleneck.conv_x_3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2464961233920 -> 2463220358496
	2463220358496 [label=AccumulateGrad]
	2463220358352 -> 2463220356960
	2463220358256 -> 2463220358160
	2464961234000 [label="layer2.2.bottleneck.bn1.weight
 (512)" fillcolor=lightblue]
	2464961234000 -> 2463220358256
	2463220358256 [label=AccumulateGrad]
	2463220358208 -> 2463220358160
	2464961234080 [label="layer2.2.bottleneck.bn1.bias
 (512)" fillcolor=lightblue]
	2464961234080 -> 2463220358208
	2463220358208 [label=AccumulateGrad]
	2463220357968 -> 2463220357824
	2464961234480 [label="layer2.2.bottleneck.conv_x_1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2464961234480 -> 2463220357968
	2463220357968 [label=AccumulateGrad]
	2463220357776 -> 2463220357728
	2464961234560 [label="layer2.2.bottleneck.bn2.weight
 (128)" fillcolor=lightblue]
	2464961234560 -> 2463220357776
	2463220357776 [label=AccumulateGrad]
	2463220357632 -> 2463220357728
	2464961234640 [label="layer2.2.bottleneck.bn2.bias
 (128)" fillcolor=lightblue]
	2464961234640 -> 2463220357632
	2463220357632 [label=AccumulateGrad]
	2463220357536 -> 2463220357392
	2464961235040 [label="layer2.2.bottleneck.conv_x_2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2464961235040 -> 2463220357536
	2463220357536 [label=AccumulateGrad]
	2463220357344 -> 2463220357296
	2464961235120 [label="layer2.2.bottleneck.bn3.weight
 (128)" fillcolor=lightblue]
	2464961235120 -> 2463220357344
	2463220357344 [label=AccumulateGrad]
	2463220357200 -> 2463220357296
	2464961235200 [label="layer2.2.bottleneck.bn3.bias
 (128)" fillcolor=lightblue]
	2464961235200 -> 2463220357200
	2463220357200 [label=AccumulateGrad]
	2463220357104 -> 2463220357008
	2464961235600 [label="layer2.2.bottleneck.conv_x_3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2464961235600 -> 2463220357104
	2463220357104 [label=AccumulateGrad]
	2463220356960 -> 2463220355568
	2463220356864 -> 2463220356768
	2464961235680 [label="layer2.3.bottleneck.bn1.weight
 (512)" fillcolor=lightblue]
	2464961235680 -> 2463220356864
	2463220356864 [label=AccumulateGrad]
	2463220356816 -> 2463220356768
	2464961235760 [label="layer2.3.bottleneck.bn1.bias
 (512)" fillcolor=lightblue]
	2464961235760 -> 2463220356816
	2463220356816 [label=AccumulateGrad]
	2463220356576 -> 2463220356432
	2464961236160 [label="layer2.3.bottleneck.conv_x_1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2464961236160 -> 2463220356576
	2463220356576 [label=AccumulateGrad]
	2463220356384 -> 2463220356336
	2464961236240 [label="layer2.3.bottleneck.bn2.weight
 (128)" fillcolor=lightblue]
	2464961236240 -> 2463220356384
	2463220356384 [label=AccumulateGrad]
	2463220356240 -> 2463220356336
	2464961236320 [label="layer2.3.bottleneck.bn2.bias
 (128)" fillcolor=lightblue]
	2464961236320 -> 2463220356240
	2463220356240 [label=AccumulateGrad]
	2463220356144 -> 2463220356000
	2464961236720 [label="layer2.3.bottleneck.conv_x_2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2464961236720 -> 2463220356144
	2463220356144 [label=AccumulateGrad]
	2463220355952 -> 2463220355904
	2464961236800 [label="layer2.3.bottleneck.bn3.weight
 (128)" fillcolor=lightblue]
	2464961236800 -> 2463220355952
	2463220355952 [label=AccumulateGrad]
	2463220355808 -> 2463220355904
	2464961236880 [label="layer2.3.bottleneck.bn3.bias
 (128)" fillcolor=lightblue]
	2464961236880 -> 2463220355808
	2463220355808 [label=AccumulateGrad]
	2463220355712 -> 2463220355616
	2464961298816 [label="layer2.3.bottleneck.conv_x_3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2464961298816 -> 2463220355712
	2463220355712 [label=AccumulateGrad]
	2463220355568 -> 2463220355424
	2463220355376 -> 2463220355328
	2464961298896 [label="layer3.0.bottleneck.bn1.weight
 (512)" fillcolor=lightblue]
	2464961298896 -> 2463220355376
	2463220355376 [label=AccumulateGrad]
	2463220355232 -> 2463220355328
	2464961298976 [label="layer3.0.bottleneck.bn1.bias
 (512)" fillcolor=lightblue]
	2464961298976 -> 2463220355232
	2463220355232 [label=AccumulateGrad]
	2463220355184 -> 2463220375408
	2464961299376 [label="layer3.0.bottleneck.conv_x_1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	2464961299376 -> 2463220355184
	2463220355184 [label=AccumulateGrad]
	2463220375360 -> 2463220375312
	2464961299456 [label="layer3.0.bottleneck.bn2.weight
 (256)" fillcolor=lightblue]
	2464961299456 -> 2463220375360
	2463220375360 [label=AccumulateGrad]
	2463220375216 -> 2463220375312
	2464961299536 [label="layer3.0.bottleneck.bn2.bias
 (256)" fillcolor=lightblue]
	2464961299536 -> 2463220375216
	2463220375216 [label=AccumulateGrad]
	2463220375120 -> 2463220374976
	2464961299936 [label="layer3.0.bottleneck.conv_x_2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2464961299936 -> 2463220375120
	2463220375120 [label=AccumulateGrad]
	2463220374928 -> 2463220374880
	2464961300016 [label="layer3.0.bottleneck.bn3.weight
 (256)" fillcolor=lightblue]
	2464961300016 -> 2463220374928
	2463220374928 [label=AccumulateGrad]
	2463220374784 -> 2463220374880
	2464961300096 [label="layer3.0.bottleneck.bn3.bias
 (256)" fillcolor=lightblue]
	2464961300096 -> 2463220374784
	2463220374784 [label=AccumulateGrad]
	2463220374688 -> 2463220374592
	2464961300496 [label="layer3.0.bottleneck.conv_x_3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2464961300496 -> 2463220374688
	2463220374688 [label=AccumulateGrad]
	2463220374544 -> 2463220373152
	2463220374544 [label=ConvolutionBackward0]
	2463203370032 -> 2463220374544
	2463203370032 [label=ReluBackward0]
	2463220375456 -> 2463203370032
	2463220375456 [label=NativeBatchNormBackward0]
	2463220355424 -> 2463220375456
	2463220375264 -> 2463220375456
	2464961300576 [label="layer3.0.bn_proj.weight
 (512)" fillcolor=lightblue]
	2464961300576 -> 2463220375264
	2463220375264 [label=AccumulateGrad]
	2463220374832 -> 2463220375456
	2464961300656 [label="layer3.0.bn_proj.bias
 (512)" fillcolor=lightblue]
	2464961300656 -> 2463220374832
	2463220374832 [label=AccumulateGrad]
	2463220375024 -> 2463220374544
	2464961301056 [label="layer3.0.projection.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	2464961301056 -> 2463220375024
	2463220375024 [label=AccumulateGrad]
	2463220374448 -> 2463220374352
	2464961301136 [label="layer3.1.bottleneck.bn1.weight
 (1024)" fillcolor=lightblue]
	2464961301136 -> 2463220374448
	2463220374448 [label=AccumulateGrad]
	2463220374400 -> 2463220374352
	2464961301216 [label="layer3.1.bottleneck.bn1.bias
 (1024)" fillcolor=lightblue]
	2464961301216 -> 2463220374400
	2463220374400 [label=AccumulateGrad]
	2463220374160 -> 2463220374016
	2464961301616 [label="layer3.1.bottleneck.conv_x_1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2464961301616 -> 2463220374160
	2463220374160 [label=AccumulateGrad]
	2463220373968 -> 2463220373920
	2464961301696 [label="layer3.1.bottleneck.bn2.weight
 (256)" fillcolor=lightblue]
	2464961301696 -> 2463220373968
	2463220373968 [label=AccumulateGrad]
	2463220373824 -> 2463220373920
	2464961301776 [label="layer3.1.bottleneck.bn2.bias
 (256)" fillcolor=lightblue]
	2464961301776 -> 2463220373824
	2463220373824 [label=AccumulateGrad]
	2463220373728 -> 2463220373584
	2464961302176 [label="layer3.1.bottleneck.conv_x_2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2464961302176 -> 2463220373728
	2463220373728 [label=AccumulateGrad]
	2463220373536 -> 2463220373488
	2464961302256 [label="layer3.1.bottleneck.bn3.weight
 (256)" fillcolor=lightblue]
	2464961302256 -> 2463220373536
	2463220373536 [label=AccumulateGrad]
	2463220373392 -> 2463220373488
	2464961302336 [label="layer3.1.bottleneck.bn3.bias
 (256)" fillcolor=lightblue]
	2464961302336 -> 2463220373392
	2463220373392 [label=AccumulateGrad]
	2463220373296 -> 2463220373200
	2464963785008 [label="layer3.1.bottleneck.conv_x_3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2464963785008 -> 2463220373296
	2463220373296 [label=AccumulateGrad]
	2463220373152 -> 2463220371760
	2463220373056 -> 2463220372960
	2464963785088 [label="layer3.2.bottleneck.bn1.weight
 (1024)" fillcolor=lightblue]
	2464963785088 -> 2463220373056
	2463220373056 [label=AccumulateGrad]
	2463220373008 -> 2463220372960
	2464963785168 [label="layer3.2.bottleneck.bn1.bias
 (1024)" fillcolor=lightblue]
	2464963785168 -> 2463220373008
	2463220373008 [label=AccumulateGrad]
	2463220372768 -> 2463220372624
	2464963785568 [label="layer3.2.bottleneck.conv_x_1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2464963785568 -> 2463220372768
	2463220372768 [label=AccumulateGrad]
	2463220372576 -> 2463220372528
	2464963785648 [label="layer3.2.bottleneck.bn2.weight
 (256)" fillcolor=lightblue]
	2464963785648 -> 2463220372576
	2463220372576 [label=AccumulateGrad]
	2463220372432 -> 2463220372528
	2464963785728 [label="layer3.2.bottleneck.bn2.bias
 (256)" fillcolor=lightblue]
	2464963785728 -> 2463220372432
	2463220372432 [label=AccumulateGrad]
	2463220372336 -> 2463220372192
	2464963786128 [label="layer3.2.bottleneck.conv_x_2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2464963786128 -> 2463220372336
	2463220372336 [label=AccumulateGrad]
	2463220372144 -> 2463220372096
	2464963786208 [label="layer3.2.bottleneck.bn3.weight
 (256)" fillcolor=lightblue]
	2464963786208 -> 2463220372144
	2463220372144 [label=AccumulateGrad]
	2463220372000 -> 2463220372096
	2464963786288 [label="layer3.2.bottleneck.bn3.bias
 (256)" fillcolor=lightblue]
	2464963786288 -> 2463220372000
	2463220372000 [label=AccumulateGrad]
	2463220371904 -> 2463220371808
	2464963786688 [label="layer3.2.bottleneck.conv_x_3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2464963786688 -> 2463220371904
	2463220371904 [label=AccumulateGrad]
	2463220371760 -> 2463202165616
	2463220371664 -> 2463202168064
	2464963786768 [label="layer3.3.bottleneck.bn1.weight
 (1024)" fillcolor=lightblue]
	2464963786768 -> 2463220371664
	2463220371664 [label=AccumulateGrad]
	2463220371616 -> 2463202168064
	2464963786848 [label="layer3.3.bottleneck.bn1.bias
 (1024)" fillcolor=lightblue]
	2464963786848 -> 2463220371616
	2463220371616 [label=AccumulateGrad]
	2463202168736 -> 2463202168016
	2464963787248 [label="layer3.3.bottleneck.conv_x_1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2464963787248 -> 2463202168736
	2463202168736 [label=AccumulateGrad]
	2463202168112 -> 2463202168448
	2464963787328 [label="layer3.3.bottleneck.bn2.weight
 (256)" fillcolor=lightblue]
	2464963787328 -> 2463202168112
	2463202168112 [label=AccumulateGrad]
	2463202168544 -> 2463202168448
	2464963787408 [label="layer3.3.bottleneck.bn2.bias
 (256)" fillcolor=lightblue]
	2464963787408 -> 2463202168544
	2463202168544 [label=AccumulateGrad]
	2463202168304 -> 2463202167968
	2464963787808 [label="layer3.3.bottleneck.conv_x_2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2464963787808 -> 2463202168304
	2463202168304 [label=AccumulateGrad]
	2463202167824 -> 2463202165472
	2464963787888 [label="layer3.3.bottleneck.bn3.weight
 (256)" fillcolor=lightblue]
	2464963787888 -> 2463202167824
	2463202167824 [label=AccumulateGrad]
	2463202167728 -> 2463202165472
	2464963787968 [label="layer3.3.bottleneck.bn3.bias
 (256)" fillcolor=lightblue]
	2464963787968 -> 2463202167728
	2463202167728 [label=AccumulateGrad]
	2463202168208 -> 2463202167776
	2464963788368 [label="layer3.3.bottleneck.conv_x_3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2464963788368 -> 2463202168208
	2463202168208 [label=AccumulateGrad]
	2463202165616 -> 2464759198048
	2463202165376 -> 2464759199488
	2464963788448 [label="layer3.4.bottleneck.bn1.weight
 (1024)" fillcolor=lightblue]
	2464963788448 -> 2463202165376
	2463202165376 [label=AccumulateGrad]
	2463202165664 -> 2464759199488
	2464963788528 [label="layer3.4.bottleneck.bn1.bias
 (1024)" fillcolor=lightblue]
	2464963788528 -> 2463202165664
	2463202165664 [label=AccumulateGrad]
	2464759200400 -> 2464759199440
	2464963858656 [label="layer3.4.bottleneck.conv_x_1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2464963858656 -> 2464759200400
	2464759200400 [label=AccumulateGrad]
	2464759197808 -> 2464759198720
	2464963858736 [label="layer3.4.bottleneck.bn2.weight
 (256)" fillcolor=lightblue]
	2464963858736 -> 2464759197808
	2464759197808 [label=AccumulateGrad]
	2464759201696 -> 2464759198720
	2464963858816 [label="layer3.4.bottleneck.bn2.bias
 (256)" fillcolor=lightblue]
	2464963858816 -> 2464759201696
	2464759201696 [label=AccumulateGrad]
	2464759199584 -> 2464759201504
	2464963859216 [label="layer3.4.bottleneck.conv_x_2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2464963859216 -> 2464759199584
	2464759199584 [label=AccumulateGrad]
	2464759201216 -> 2464759201168
	2464963859296 [label="layer3.4.bottleneck.bn3.weight
 (256)" fillcolor=lightblue]
	2464963859296 -> 2464759201216
	2464759201216 [label=AccumulateGrad]
	2464759201264 -> 2464759201168
	2464963859376 [label="layer3.4.bottleneck.bn3.bias
 (256)" fillcolor=lightblue]
	2464963859376 -> 2464759201264
	2464759201264 [label=AccumulateGrad]
	2464759201648 -> 2464759198240
	2464963859776 [label="layer3.4.bottleneck.conv_x_3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2464963859776 -> 2464759201648
	2464759201648 [label=AccumulateGrad]
	2464759198048 -> 2464759191056
	2464759198144 -> 2464759198576
	2464963859856 [label="layer3.5.bottleneck.bn1.weight
 (1024)" fillcolor=lightblue]
	2464963859856 -> 2464759198144
	2464759198144 [label=AccumulateGrad]
	2464759201744 -> 2464759198576
	2464963859936 [label="layer3.5.bottleneck.bn1.bias
 (1024)" fillcolor=lightblue]
	2464963859936 -> 2464759201744
	2464759201744 [label=AccumulateGrad]
	2464759197856 -> 2464759200688
	2464963860336 [label="layer3.5.bottleneck.conv_x_1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2464963860336 -> 2464759197856
	2464759197856 [label=AccumulateGrad]
	2464759199920 -> 2464759199968
	2464963860416 [label="layer3.5.bottleneck.bn2.weight
 (256)" fillcolor=lightblue]
	2464963860416 -> 2464759199920
	2464759199920 [label=AccumulateGrad]
	2464759199680 -> 2464759199968
	2464963860496 [label="layer3.5.bottleneck.bn2.bias
 (256)" fillcolor=lightblue]
	2464963860496 -> 2464759199680
	2464759199680 [label=AccumulateGrad]
	2464759201408 -> 2464759200304
	2464963860896 [label="layer3.5.bottleneck.conv_x_2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2464963860896 -> 2464759201408
	2464759201408 [label=AccumulateGrad]
	2464759200016 -> 2464759200352
	2464963860976 [label="layer3.5.bottleneck.bn3.weight
 (256)" fillcolor=lightblue]
	2464963860976 -> 2464759200016
	2464759200016 [label=AccumulateGrad]
	2464759198672 -> 2464759200352
	2464963861056 [label="layer3.5.bottleneck.bn3.bias
 (256)" fillcolor=lightblue]
	2464963861056 -> 2464759198672
	2464759198672 [label=AccumulateGrad]
	2464759192208 -> 2464759190480
	2464963861456 [label="layer3.5.bottleneck.conv_x_3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2464963861456 -> 2464759192208
	2464759192208 [label=AccumulateGrad]
	2464759191056 -> 2463199174912
	2463199177648 -> 2463224180400
	2464963861536 [label="layer4.0.bottleneck.bn1.weight
 (1024)" fillcolor=lightblue]
	2464963861536 -> 2463199177648
	2463199177648 [label=AccumulateGrad]
	2463224211536 -> 2463224180400
	2464963861616 [label="layer4.0.bottleneck.bn1.bias
 (1024)" fillcolor=lightblue]
	2464963861616 -> 2463224211536
	2463224211536 [label=AccumulateGrad]
	2463224211056 -> 2463199273024
	2464963862016 [label="layer4.0.bottleneck.conv_x_1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	2464963862016 -> 2463224211056
	2463224211056 [label=AccumulateGrad]
	2463224211248 -> 2463199273072
	2464963862096 [label="layer4.0.bottleneck.bn2.weight
 (512)" fillcolor=lightblue]
	2464963862096 -> 2463224211248
	2463224211248 [label=AccumulateGrad]
	2463224211392 -> 2463199273072
	2464963862176 [label="layer4.0.bottleneck.bn2.bias
 (512)" fillcolor=lightblue]
	2464963862176 -> 2463224211392
	2463224211392 [label=AccumulateGrad]
	2463199275280 -> 2463222525136
	2464963924112 [label="layer4.0.bottleneck.conv_x_2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2464963924112 -> 2463199275280
	2463199275280 [label=AccumulateGrad]
	2463222523888 -> 2463224171392
	2464963924192 [label="layer4.0.bottleneck.bn3.weight
 (512)" fillcolor=lightblue]
	2464963924192 -> 2463222523888
	2463222523888 [label=AccumulateGrad]
	2463222525664 -> 2463224171392
	2464963924272 [label="layer4.0.bottleneck.bn3.bias
 (512)" fillcolor=lightblue]
	2464963924272 -> 2463222525664
	2463222525664 [label=AccumulateGrad]
	2463199554528 -> 2463199555440
	2464963924672 [label="layer4.0.bottleneck.conv_x_3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2464963924672 -> 2463199554528
	2463199554528 [label=AccumulateGrad]
	2463199555248 -> 2463199552368
	2463199555248 [label=ConvolutionBackward0]
	2463224210480 -> 2463199555248
	2463224210480 [label=ReluBackward0]
	2463224171008 -> 2463224210480
	2463224171008 [label=NativeBatchNormBackward0]
	2463199174912 -> 2463224171008
	2463224176992 -> 2463224171008
	2464963924752 [label="layer4.0.bn_proj.weight
 (1024)" fillcolor=lightblue]
	2464963924752 -> 2463224176992
	2463224176992 [label=AccumulateGrad]
	2463199273312 -> 2463224171008
	2464963924832 [label="layer4.0.bn_proj.bias
 (1024)" fillcolor=lightblue]
	2464963924832 -> 2463199273312
	2463199273312 [label=AccumulateGrad]
	2463224211920 -> 2463199555248
	2464963925232 [label="layer4.0.projection.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	2464963925232 -> 2463224211920
	2463224211920 [label=AccumulateGrad]
	2463199554864 -> 2463199555344
	2464963925312 [label="layer4.1.bottleneck.bn1.weight
 (2048)" fillcolor=lightblue]
	2464963925312 -> 2463199554864
	2463199554864 [label=AccumulateGrad]
	2463199554576 -> 2463199555344
	2464963925392 [label="layer4.1.bottleneck.bn1.bias
 (2048)" fillcolor=lightblue]
	2464963925392 -> 2463199554576
	2463199554576 [label=AccumulateGrad]
	2463199553568 -> 2463199554144
	2464963925792 [label="layer4.1.bottleneck.conv_x_1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2464963925792 -> 2463199553568
	2463199553568 [label=AccumulateGrad]
	2463199552560 -> 2463199552992
	2464963925872 [label="layer4.1.bottleneck.bn2.weight
 (512)" fillcolor=lightblue]
	2464963925872 -> 2463199552560
	2463199552560 [label=AccumulateGrad]
	2463199552032 -> 2463199552992
	2464963925952 [label="layer4.1.bottleneck.bn2.bias
 (512)" fillcolor=lightblue]
	2464963925952 -> 2463199552032
	2463199552032 [label=AccumulateGrad]
	2463199552224 -> 2463199554384
	2464963926352 [label="layer4.1.bottleneck.conv_x_2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2464963926352 -> 2463199552224
	2463199552224 [label=AccumulateGrad]
	2463199552272 -> 2463199554720
	2464963926432 [label="layer4.1.bottleneck.bn3.weight
 (512)" fillcolor=lightblue]
	2464963926432 -> 2463199552272
	2463199552272 [label=AccumulateGrad]
	2463199552464 -> 2463199554720
	2464963926512 [label="layer4.1.bottleneck.bn3.bias
 (512)" fillcolor=lightblue]
	2464963926512 -> 2463199552464
	2463199552464 [label=AccumulateGrad]
	2463199551744 -> 2463199554912
	2464963926912 [label="layer4.1.bottleneck.conv_x_3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2464963926912 -> 2463199551744
	2463199551744 [label=AccumulateGrad]
	2463199552368 -> 2463203371424
	2463199553328 -> 2463202157760
	2464963926992 [label="layer4.2.bottleneck.bn1.weight
 (2048)" fillcolor=lightblue]
	2464963926992 -> 2463199553328
	2463199553328 [label=AccumulateGrad]
	2463199552704 -> 2463202157760
	2464963927072 [label="layer4.2.bottleneck.bn1.bias
 (2048)" fillcolor=lightblue]
	2464963927072 -> 2463199552704
	2463199552704 [label=AccumulateGrad]
	2463202160400 -> 2463202158336
	2464963927472 [label="layer4.2.bottleneck.conv_x_1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2464963927472 -> 2463202160400
	2463202160400 [label=AccumulateGrad]
	2463202159152 -> 2463202158672
	2464963927552 [label="layer4.2.bottleneck.bn2.weight
 (512)" fillcolor=lightblue]
	2464963927552 -> 2463202159152
	2463202159152 [label=AccumulateGrad]
	2463202159200 -> 2463202158672
	2464963927632 [label="layer4.2.bottleneck.bn2.bias
 (512)" fillcolor=lightblue]
	2464963927632 -> 2463202159200
	2463202159200 [label=AccumulateGrad]
	2463202160544 -> 2463202156896
	2464963977280 [label="layer4.2.bottleneck.conv_x_2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2464963977280 -> 2463202160544
	2463202160544 [label=AccumulateGrad]
	2463202160160 -> 2463198531744
	2464963977360 [label="layer4.2.bottleneck.bn3.weight
 (512)" fillcolor=lightblue]
	2464963977360 -> 2463202160160
	2463202160160 [label=AccumulateGrad]
	2463202159440 -> 2463198531744
	2464963977440 [label="layer4.2.bottleneck.bn3.bias
 (512)" fillcolor=lightblue]
	2464963977440 -> 2463202159440
	2463202159440 [label=AccumulateGrad]
	2463203372336 -> 2463203372096
	2464963977840 [label="layer4.2.bottleneck.conv_x_3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2464963977840 -> 2463203372336
	2463203372336 [label=AccumulateGrad]
	2463203371424 -> 2463203370560
	2463203371856 -> 2463203371040
	2464963977920 [label="post_bn.weight
 (2048)" fillcolor=lightblue]
	2464963977920 -> 2463203371856
	2463203371856 [label=AccumulateGrad]
	2463203371520 -> 2463203371040
	2464963978000 [label="post_bn.bias
 (2048)" fillcolor=lightblue]
	2464963978000 -> 2463203371520
	2463203371520 [label=AccumulateGrad]
	2463203371232 -> 2463203371904
	2463203371232 [label=TBackward0]
	2463203371664 -> 2463203371232
	2464963978320 [label="linear.weight
 (1000, 2048)" fillcolor=lightblue]
	2464963978320 -> 2463203371664
	2463203371664 [label=AccumulateGrad]
	2463203370608 -> 2463203371568
	2464963978400 [label="linear.bias
 (1000)" fillcolor=lightblue]
	2464963978400 -> 2463203370608
	2463203370608 [label=AccumulateGrad]
	2463203371568 -> 2465059342752
}
